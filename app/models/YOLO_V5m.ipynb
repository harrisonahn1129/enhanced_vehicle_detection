{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e717d3e5",
   "metadata": {},
   "source": [
    "# YOLO_V5m Model\n",
    "\n",
    "For this project, pre-trained YOLO-V5m model will be used as the base model for comparing the vehicle detection quality.\n",
    "\n",
    "As a base model, pre-trained YOLO-V5m will be utilized to detect vehicles on the highway video clip. Later in the YOLO_SwinV2.ipynb, the modified YOLO model will be trained and tested to detect the vehicles on the same highway video clip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b1a51",
   "metadata": {},
   "source": [
    "## How To Run\n",
    "\n",
    "It is recommended to run this notebook in Google Colab. However, it is implemented so that it can also be run in a local environment.\n",
    "\n",
    "**To run this notebook in Google Colab:**\n",
    "- Download the whole project folder (enhanced_vehicle_detection) from GitHub.\n",
    "- Place it in MyDrive in Google Drive.\n",
    "    - If the project folder is placed in a different path in Google Drive, the paths for the input video and outputs need to be edited accordingly.\n",
    "- All set! You can now run the cells.\n",
    "\n",
    "**To run this notebook in a local environment:**\n",
    "- Fork or clone the GitHub repository.\n",
    "- Run `pip install -r app/requirements.txt` to install all required libraries.\n",
    "- Since the code requires video conversion, make sure to install **ffmpeg**:\n",
    "    - macOS: `brew install ffmpeg`\n",
    "    - Ubuntu/Linux: `sudo apt install ffmpeg`\n",
    "    - Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html)\n",
    "- All set! You can now run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66377c",
   "metadata": {},
   "source": [
    "## Setup YOLO V5 \n",
    "\n",
    "The code below installs every required libraries to load and use YOLO-V5 model. This code only need to be run once while using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04323edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -q https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!pip -q install -r requirements.txt opencv-python-headless==4.10.0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d07bc3",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f43f0",
   "metadata": {},
   "source": [
    "## Loading Pre-Trained YOLO-V5m model and Setup configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    # For Google Colab:\n",
    "    # Mount Google Drive in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set the video path to your Google Drive location\n",
    "    # Update this path to match where you stored the video in Google Drive\n",
    "    video_path = '/content/drive/MyDrive/enhanced_vehicle_detection/data/rainy_highway_video.mp4'\n",
    "else:\n",
    "    # For local environment: use relative path\n",
    "    video_path = '../data/rainy_highway_video.mp4'\n",
    "\n",
    "# Verify the video file exists\n",
    "if not os.path.exists(video_path):\n",
    "    raise FileNotFoundError(f\"Video not found at: {video_path}\\n\"\n",
    "                            f\"Please ensure the video is at the correct location.\")\n",
    "\n",
    "# Setting the FPS, confident score threshold, and IOU match threshold\n",
    "FPS = 25\n",
    "CONF_THRESH = 0.4\n",
    "IOU_MATCH_THRESH = 0.3\n",
    "\n",
    "# Specifying the classes we are interested to detect from the pre-trained classes\n",
    "VEHICLE_CLASSES = {\"car\", \"truck\", \"bus\"}\n",
    "\n",
    "# Loading the pre-trained YOLO-V5m\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
    "\n",
    "# Filtering the weak detection by setting the model's confident score to CONF_THRESH\n",
    "# and setting the model's internal NMS IOU threshold to 0.45\n",
    "yolo_model.conf = CONF_THRESH\n",
    "yolo_model.iou = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019c9cd",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The helper functions for detecting vehicles, computing IoU, and updating tracking line for each detection are implemented.\n",
    "\n",
    "- **detect_vehicles(frame)**: It will pass each frame to YOLOY-V5m and utilize the bbox, confident score, and class label returned from the model. The returned values will be used to visualize on the input video.\n",
    "- **iou_xyxy(boxA, boxB)**: It will compute IoU of between every detection. The computed IoU is applied with the threshold for tracking.\n",
    "    - If IoU is greater than the threshold, the detection is assigned to the same track. Otherwise, it will create a new track for the detection with the new track ID.\n",
    "- **update_tracks(detections, flow, frame_index)**: It will update the tracking for each detection using bbox, list of the previous center points, track ID, and previoud frame. IoU computation and thresholding is also used in this helper function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd63a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vehicles(frame):\n",
    "  # Pass RGB images to YOLO-V5\n",
    "  results = yolo_model(frame[:, :, ::-1])\n",
    "\n",
    "  detections = []\n",
    "\n",
    "  for *xyxy, conf, cls in results.xyxy[0].cpu().numpy():\n",
    "    # Obtain bbox coordinates\n",
    "    x1, y1, x2, y2 = map(int, xyxy)\n",
    "    cls_id = int(cls)\n",
    "    # Obtain class name\n",
    "    cls_name = results.names[cls_id]\n",
    "\n",
    "    if cls_name not in VEHICLE_CLASSES:\n",
    "      continue\n",
    "\n",
    "    detections.append(\n",
    "        {\n",
    "            \"bbox\": [x1, y1, x2, y2],\n",
    "            \"conf\": float(conf),\n",
    "            \"class\": cls_name\n",
    "        }\n",
    "    )\n",
    "\n",
    "  return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153faaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_xyxy(boxA, boxB):\n",
    "  \"\"\"\n",
    "  IoU between two boxes in (x1,y1,x2,y2) format.\n",
    "  \"\"\"\n",
    "  xA = max(boxA[0], boxB[0])\n",
    "  yA = max(boxA[1], boxB[1])\n",
    "  xB = min(boxA[2], boxB[2])\n",
    "  yB = min(boxA[3], boxB[3])\n",
    "\n",
    "  interW = max(0, xB - xA)\n",
    "  interH = max(0, yB - yA)\n",
    "  interArea = interW * interH\n",
    "\n",
    "  if interArea == 0:\n",
    "      return 0.0\n",
    "\n",
    "  areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "  areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "  return interArea / float(areaA + areaB - interArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeae005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracks(detections, flow, frame_index):\n",
    "  \"\"\"\n",
    "  detections: list from detect_vehicles\n",
    "  flow: dense optical flow from prev->current frame (HxWx2)\n",
    "  frame_index: current frame number (int)\n",
    "  \"\"\"\n",
    "  global tracks, next_track_id\n",
    "\n",
    "  # Initially, no detections are assigned, all tracks are unmatched\n",
    "  unmatched_tracks = set(tracks.keys())\n",
    "  det_to_track = {}\n",
    "\n",
    "  # IoU-based matching\n",
    "  for det_idx, det in enumerate(detections):\n",
    "    box_det = det[\"bbox\"]\n",
    "    best_iou = 0\n",
    "    best_track = None\n",
    "\n",
    "    for tid in unmatched_tracks:\n",
    "      box_tr = tracks[tid][\"bbox\"]\n",
    "      iou_val = iou_xyxy(box_det, box_tr)\n",
    "      if iou_val > best_iou:\n",
    "        best_iou = iou_val\n",
    "        best_track = tid\n",
    "\n",
    "    if best_iou > IOU_MATCH_THRESH:\n",
    "      det_to_track[det_idx] = best_track\n",
    "      unmatched_tracks.remove(best_track)\n",
    "\n",
    "  # Update matched tracks (bbox, trace, speed)\n",
    "  H, W = flow.shape[:2]\n",
    "\n",
    "  for det_idx, track_id in det_to_track.items():\n",
    "    x1, y1, x2, y2 = detections[det_idx][\"bbox\"]\n",
    "\n",
    "    cx = (x1 + x2) // 2\n",
    "    cy = (y1 + y2) // 2\n",
    "\n",
    "    tracks[track_id][\"bbox\"] = (x1, y1, x2, y2)\n",
    "    tracks[track_id][\"trace\"].append((cx, cy))\n",
    "    tracks[track_id][\"last_seen\"] = frame_index\n",
    "\n",
    "  # Create new tracks for unmatched detections\n",
    "  for det_idx, det in enumerate(detections):\n",
    "    if det_idx in det_to_track:\n",
    "      continue\n",
    "\n",
    "    x1, y1, x2, y2 = det[\"bbox\"]\n",
    "    cx = (x1 + x2) // 2\n",
    "    cy = (y1 + y2) // 2\n",
    "\n",
    "    tracks[next_track_id] = {\n",
    "      \"bbox\": (x1, y1, x2, y2),\n",
    "      \"trace\": [(cx, cy)],\n",
    "      \"last_seen\": frame_index\n",
    "    }\n",
    "    next_track_id += 1\n",
    "\n",
    "  # Remove tracks that disappeared\n",
    "  max_missing_frames = FPS\n",
    "  tracks = {\n",
    "    tid: tr for tid, tr in tracks.items()\n",
    "    if frame_index - tr[\"last_seen\"] <= max_missing_frames\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391017cb",
   "metadata": {},
   "source": [
    "## Main Run\n",
    "\n",
    "Now utilizing every helper functions and configurations above, the main code below will\n",
    "\n",
    "- Read the input video frame by frame and initialize the video writer for the output\n",
    "- Detect vehicles in each frame using the pre-trained YOLO-V5m model\n",
    "- Collect inference metrics for each frame including:\n",
    "    - Number of detections\n",
    "    - Confidence scores for all detections\n",
    "    - Detection counts by class (car, truck, bus)\n",
    "- Compute dense optical flow between consecutive frames using the Farneback method\n",
    "- Update tracking for each detection by matching detections with existing tracks using IoU threshold\n",
    "- Draw bounding boxes, track IDs, and tracking lines on each frame for visualization\n",
    "- Write the annotated frames to an output video file\n",
    "- Convert the output video to MP4 format using ffmpeg and display it in the notebook\n",
    "\n",
    "### Farneback Optical Flow\n",
    "\n",
    "The Farneback method is a dense optical flow algorithm that estimates motion vectors for every pixel between two consecutive frames. Unlike sparse methods (e.g., Lucas-Kanade) that track specific feature points, Farneback computes a complete motion field across the entire image.\n",
    "\n",
    "#### How it works:\n",
    "- Approximates each neighborhood of pixels using polynomial expansion\n",
    "- Estimates displacement by analyzing how these polynomials change between frames\n",
    "- Uses a multi-scale pyramid approach to handle large motions\n",
    "\n",
    "#### Parameters used in cv2.calcOpticalFlowFarneback:\n",
    "- **pyr_scale=0.5**: Each pyramid level is half the size of the previous\n",
    "- **levels=3**: Number of pyramid levels\n",
    "- **winsize=15**: Averaging window size (larger = smoother but less precise)\n",
    "- **iterations=3**: Number of iterations at each pyramid level\n",
    "- **poly_n=5**: Size of pixel neighborhood for polynomial expansion\n",
    "- **poly_sigma=1.2**: Standard deviation of Gaussian for polynomial expansion\n",
    "\n",
    "The resulting flow field helps improve vehicle tracking by providing motion context between frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "  raise RuntimeError(\"Couldn't read the video\")\n",
    "\n",
    "h, w = frame.shape[:2]\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 15\n",
    "\n",
    "out = cv2.VideoWriter(\n",
    "  \"YOLO_V5m_highway_with_detection.avi\",\n",
    "  cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "  fps,\n",
    "  (w, h)\n",
    ")\n",
    "\n",
    "old_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "tracks = {}\n",
    "next_track_id = 0\n",
    "\n",
    "# Collect inference metrics\n",
    "metrics = {\n",
    "    \"frame_indices\": [],\n",
    "    \"detections_per_frame\": [],\n",
    "    \"confidence_scores\": [],\n",
    "    \"class_counts\": {\"car\": [], \"truck\": [], \"bus\": []},\n",
    "}\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "frame_index = 0\n",
    "\n",
    "while True:\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "      break\n",
    "\n",
    "  frame_index += 1\n",
    "\n",
    "  dets = detect_vehicles(frame)\n",
    "\n",
    "  # Collect metrics for this frame\n",
    "  metrics[\"frame_indices\"].append(frame_index)\n",
    "  metrics[\"detections_per_frame\"].append(len(dets))\n",
    "\n",
    "  # Count detections by class and collect confidence scores\n",
    "  class_count = {\"car\": 0, \"truck\": 0, \"bus\": 0}\n",
    "  for det in dets:\n",
    "    metrics[\"confidence_scores\"].append(det[\"conf\"])\n",
    "    class_count[det[\"class\"]] += 1\n",
    "\n",
    "  for cls in class_count:\n",
    "    metrics[\"class_counts\"][cls].append(class_count[cls])\n",
    "\n",
    "  frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Calculate Dense Optical Flow\n",
    "  # This is the core function\n",
    "  flow = cv2.calcOpticalFlowFarneback(old_gray, frame_gray,\n",
    "                                      None,\n",
    "                                      0.5,  # pyr_scale\n",
    "                                      3,    # levels\n",
    "                                      15,   # winsize\n",
    "                                      3,    # iterations\n",
    "                                      5,    # poly_n\n",
    "                                      1.2,  # poly_sigma\n",
    "                                      0)    # flags\n",
    "\n",
    "  update_tracks(dets, flow, frame_index)\n",
    "\n",
    "  # Draw track ID and track line for each detected car\n",
    "  for track_id, track in tracks.items():\n",
    "    x1, y1, x2, y2 = track[\"bbox\"]\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "    # Write track ID\n",
    "    cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,225,0), 2)\n",
    "\n",
    "    # Draw track line\n",
    "    if len(track[\"trace\"]) > 1:\n",
    "      line = np.array(track[\"trace\"], dtype=np.int32).reshape(-1,1,2)\n",
    "      cv2.polylines(frame, [line], False, (0,225,255), 2)\n",
    "\n",
    "\n",
    "  out.write(frame)\n",
    "\n",
    "  old_gray = frame_gray.copy()\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Vehicle Detection video for Part B\")\n",
    "\n",
    "# Set output path based on environment\n",
    "if IN_COLAB:\n",
    "    output_path_part_b = '/content/drive/MyDrive/enhanced_vehicle_detection/outputs/YOLO_V5m/highway_with_detection.mp4'\n",
    "else:\n",
    "    output_dir = '../outputs/YOLO_V5m'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_path_part_b = os.path.join(output_dir, 'highway_with_detection.mp4')\n",
    "\n",
    "!ffmpeg -y -i YOLO_V5m_highway_with_detection.avi -vcodec libx264 -crf 23 -pix_fmt yuv420p {output_path_part_b} >/dev/null 2>&1\n",
    "\n",
    "part_b_mp4 = open(output_path_part_b,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(part_b_mp4).decode()\n",
    "HTML(f'<video width=640 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec170e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Inference Metrics Visualization\n",
    "\n",
    "The following visualizations show the inference metrics collected during vehicle detection. These metrics can be compared with the YOLO-SwinV2 model to evaluate performance differences.\n",
    "\n",
    "- **Detections per Frame**: Number of vehicles detected in each frame over time\n",
    "- **Confidence Score Distribution**: Histogram showing the distribution of detection confidence scores\n",
    "- **Class Distribution**: Breakdown of detections by vehicle class (car, truck, bus)\n",
    "- **Summary Statistics**: Overall metrics including total detections and mean confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a770df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visualization output directory based on environment\n",
    "if IN_COLAB:\n",
    "    viz_dir = '/content/drive/MyDrive/enhanced_vehicle_detection/outputs/YOLO_V5m/visualizations'\n",
    "else:\n",
    "    viz_dir = '../outputs/YOLO_V5m/visualizations'\n",
    "\n",
    "# Create visualization directory if it doesn't exist\n",
    "if not os.path.exists(viz_dir):\n",
    "    os.makedirs(viz_dir)\n",
    "    print(f\"Created directory: {viz_dir}\")\n",
    "\n",
    "# Detections per Frame\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax1.plot(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], color='#2E86AB', linewidth=1.5)\n",
    "ax1.fill_between(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], alpha=0.3, color='#2E86AB')\n",
    "ax1.set_xlabel('Frame Index')\n",
    "ax1.set_ylabel('Number of Detections')\n",
    "ax1.set_title('Detections per Frame')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_dir, 'detections_per_frame.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confidence Score Distribution\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "if metrics[\"confidence_scores\"]:\n",
    "    ax2.hist(metrics[\"confidence_scores\"], bins=30, color='#A23B72', edgecolor='white', alpha=0.8)\n",
    "    ax2.axvline(np.mean(metrics[\"confidence_scores\"]), color='#F18F01', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {np.mean(metrics[\"confidence_scores\"]):.3f}')\n",
    "    ax2.legend()\n",
    "ax2.set_xlabel('Confidence Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Confidence Score Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_dir, 'confidence_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Class Distribution (Stacked Area)\n",
    "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "frames = metrics[\"frame_indices\"]\n",
    "car_counts = metrics[\"class_counts\"][\"car\"]\n",
    "truck_counts = metrics[\"class_counts\"][\"truck\"]\n",
    "bus_counts = metrics[\"class_counts\"][\"bus\"]\n",
    "ax3.stackplot(frames, car_counts, truck_counts, bus_counts, \n",
    "              labels=['Car', 'Truck', 'Bus'],\n",
    "              colors=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.set_xlabel('Frame Index')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Detection Count by Class')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_dir, 'class_distribution_over_time.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Total Class Distribution (Pie Chart)\n",
    "fig4, ax4 = plt.subplots(figsize=(8, 8))\n",
    "total_cars = sum(car_counts)\n",
    "total_trucks = sum(truck_counts)\n",
    "total_buses = sum(bus_counts)\n",
    "sizes = [total_cars, total_trucks, total_buses]\n",
    "labels = [f'Car\\n({total_cars})', f'Truck\\n({total_trucks})', f'Bus\\n({total_buses})']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "ax4.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax4.set_title('Total Detection Distribution by Class')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_dir, 'class_distribution_pie.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Combined Summary Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('YOLO-V5m Inference Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0, 0].plot(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], color='#2E86AB', linewidth=1.5)\n",
    "axes[0, 0].fill_between(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], alpha=0.3, color='#2E86AB')\n",
    "axes[0, 0].set_xlabel('Frame Index')\n",
    "axes[0, 0].set_ylabel('Number of Detections')\n",
    "axes[0, 0].set_title('Detections per Frame')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "if metrics[\"confidence_scores\"]:\n",
    "    axes[0, 1].hist(metrics[\"confidence_scores\"], bins=30, color='#A23B72', edgecolor='white', alpha=0.8)\n",
    "    axes[0, 1].axvline(np.mean(metrics[\"confidence_scores\"]), color='#F18F01', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {np.mean(metrics[\"confidence_scores\"]):.3f}')\n",
    "    axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel('Confidence Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Confidence Score Distribution')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].stackplot(frames, car_counts, truck_counts, bus_counts, \n",
    "              labels=['Car', 'Truck', 'Bus'],\n",
    "              colors=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)\n",
    "axes[1, 0].legend(loc='upper right')\n",
    "axes[1, 0].set_xlabel('Frame Index')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Detection Count by Class')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "axes[1, 1].set_title('Total Detection Distribution by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_dir, 'metrics_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print Summary Statistics\n",
    "total_frames = len(metrics[\"frame_indices\"])\n",
    "total_detections = sum(metrics[\"detections_per_frame\"])\n",
    "avg_detections = np.mean(metrics[\"detections_per_frame\"])\n",
    "avg_confidence = np.mean(metrics[\"confidence_scores\"]) if metrics[\"confidence_scores\"] else 0\n",
    "std_confidence = np.std(metrics[\"confidence_scores\"]) if metrics[\"confidence_scores\"] else 0\n",
    "\n",
    "print()\n",
    "print(\"YOLO-V5m Summary Statistics\")\n",
    "print(f\"\\nDetection Statistics\")\n",
    "print(f\"   Total Frames Processed:  {total_frames}\")\n",
    "print(f\"   Total Detections:        {total_detections}\")\n",
    "print(f\"   Avg Detections/Frame:    {avg_detections:.2f}\")\n",
    "print(f\"\\nConfidence Statistics\")\n",
    "print(f\"   Mean Confidence:         {avg_confidence:.4f}\")\n",
    "print(f\"   Std Confidence:          {std_confidence:.4f}\")\n",
    "print(f\"   Min Confidence:          {min(metrics['confidence_scores']) if metrics['confidence_scores'] else 0:.4f}\")\n",
    "print(f\"   Max Confidence:          {max(metrics['confidence_scores']) if metrics['confidence_scores'] else 0:.4f}\")\n",
    "\n",
    "print(f\"\\nVisualizations saved to '{viz_dir}/':\")\n",
    "print(\"  - detections_per_frame.png\")\n",
    "print(\"  - confidence_distribution.png\")\n",
    "print(\"  - class_distribution_over_time.png\")\n",
    "print(\"  - class_distribution_pie.png\")\n",
    "print(\"  - metrics_summary.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad88b19",
   "metadata": {},
   "source": [
    "## Exporting Inference Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Export metrics to JSON for comparison with YOLO-SwinV2\n",
    "metrics_export = {\n",
    "    \"model_name\": \"YOLO-V5m (Pre-trained)\",\n",
    "    \"config\": {\n",
    "        \"conf_threshold\": CONF_THRESH,\n",
    "        \"iou_match_threshold\": IOU_MATCH_THRESH,\n",
    "        \"vehicle_classes\": list(VEHICLE_CLASSES)\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_frames\": len(metrics[\"frame_indices\"]),\n",
    "        \"total_detections\": sum(metrics[\"detections_per_frame\"]),\n",
    "        \"avg_detections_per_frame\": float(np.mean(metrics[\"detections_per_frame\"])),\n",
    "        \"mean_confidence\": float(np.mean(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
    "        \"std_confidence\": float(np.std(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
    "        \"min_confidence\": float(min(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
    "        \"max_confidence\": float(max(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
    "        \"total_cars\": sum(metrics[\"class_counts\"][\"car\"]),\n",
    "        \"total_trucks\": sum(metrics[\"class_counts\"][\"truck\"]),\n",
    "        \"total_buses\": sum(metrics[\"class_counts\"][\"bus\"])\n",
    "    },\n",
    "    \"per_frame_data\": {\n",
    "        \"frame_indices\": metrics[\"frame_indices\"],\n",
    "        \"detections_per_frame\": metrics[\"detections_per_frame\"],\n",
    "        \"class_counts\": metrics[\"class_counts\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"../outputs/YOLO_V5m\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "metrics_path = os.path.join(output_dir, \"YOLO_V5m_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics_export, f, indent=2)\n",
    "\n",
    "print(f\"Metrics exported to '{metrics_path}'\")\n",
    "print(\"\\nYou can load this file in YOLO_SwinV2.ipynb to compare performance.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
