{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6c361898",
      "metadata": {
        "id": "6c361898"
      },
      "source": [
        "# Vehicle Detection with YOLO-SwinV2 Model\n",
        "\n",
        "This notebook implements a modified YOLO model with SwinV2-Tiny as the backbone for vehicle detection. The model is trained on the AAU RainSnow dataset (vehicles in rainy/snowy conditions) and evaluated on the same highway video as the pre-trained YOLO-V5m model.\n",
        "\n",
        "**Key Features:**\n",
        "- SwinV2-Tiny backbone replacing the original CSPDarknet backbone\n",
        "- Training on weather-degraded vehicle data for improved detection in adverse conditions\n",
        "- Metrics collection for comparison with the pre-trained YOLO-V5m baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e15d03",
      "metadata": {
        "id": "d0e15d03"
      },
      "source": [
        "## How To Run\n",
        "\n",
        "It is recommended to run this notebook in Google Colab. However, it is implemented so that it can also be run in a local environment.\n",
        "\n",
        "**To run this notebook in Google Colab:**\n",
        "- Download the whole project folder (enhanced_vehicle_detection) from GitHub.\n",
        "- Place it in MyDrive in Google Drive.\n",
        "    - If the project folder is placed in a different path in Google Drive, the paths for the input video and outputs need to be edited accordingly.\n",
        "- All set! You can now run the cells.\n",
        "\n",
        "**To run this notebook in a local environment:**\n",
        "- Fork or clone the GitHub repository.\n",
        "- Run `pip install -r app/requirements.txt` to install all required libraries.\n",
        "- Since the code requires video conversion, make sure to install **ffmpeg**:\n",
        "    - macOS: `brew install ffmpeg`\n",
        "    - Ubuntu/Linux: `sudo apt install ffmpeg`\n",
        "    - Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html)\n",
        "- All set! You can now run the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e60300e",
      "metadata": {
        "id": "5e60300e"
      },
      "source": [
        "## Setup YOLO V5\n",
        "\n",
        "The code below installs every required libraries to load and use YOLO-V5 model. This code only need to be run once while using this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Jp2lgmXRjRlS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp2lgmXRjRlS",
        "outputId": "db5ef678-ca6d-4b50-9a55-f136e348731d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone -q https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "\n",
        "!pip install -q -r requirements.txt opencv-python-headless==4.10.0.84 timm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51co0qE1jbl5",
      "metadata": {
        "id": "51co0qE1jbl5"
      },
      "source": [
        "## Clean YOLO V5 directory\n",
        "\n",
        "If there is any old patches applied to the original YOLO V5 files, remove them and restore back to the original file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yAR_TTZsjaHn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAR_TTZsjaHn",
        "outputId": "f0c51cf2-2d11-494b-96a6-73ac87b7d3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'yolov5'\n",
            "/content/yolov5\n",
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ],
      "source": [
        "%cd yolov5\n",
        "!git status\n",
        "\n",
        "# Reset modified core files (safe and important)\n",
        "!git checkout -- models/yolo.py models/common.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a5b7125",
      "metadata": {
        "id": "1a5b7125"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e893fec7",
      "metadata": {
        "id": "e893fec7"
      },
      "outputs": [],
      "source": [
        "import cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "import timm\n",
        "import json, shutil\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7b7ced",
      "metadata": {
        "id": "bb7b7ced"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up paths based on whether running in Google Colab or local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ab2bf134",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab2bf134",
        "outputId": "a66a0b17-d627-4035-bb66-0805439a2081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Random seed set to: 42\n",
            "Using device: cuda\n",
            "Data root: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/training_data_vehicles_in_rain\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Google Colab\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Paths for Colab\n",
        "    DATA_ROOT = Path('/content/drive/MyDrive/DL/enhanced_vehicle_detection/data/training_data_vehicles_in_rain')\n",
        "    VIDEO_PATH = Path('/content/drive/MyDrive/DL/enhanced_vehicle_detection/data/rainy_highway_video.mp4')\n",
        "    PROJECT_ROOT = Path('/content/drive/MyDrive/DL/enhanced_vehicle_detection')\n",
        "else:\n",
        "    # Paths for local environment\n",
        "    DATA_ROOT = Path('../data/training_data_vehicles_in_rain')\n",
        "    VIDEO_PATH = Path('../data/rainy_highway_video.mp4')\n",
        "    PROJECT_ROOT = Path('../')\n",
        "\n",
        "# ============================================================================\n",
        "# RANDOM SEED FOR REPRODUCIBILITY\n",
        "# ============================================================================\n",
        "SEED = 42  # Change this value to get different reproducible results\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
        "\n",
        "    # For deterministic behavior (may slow down training slightly)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Set environment variable for additional reproducibility\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Random seed set to: {SEED}\")\n",
        "# ============================================================================\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZNZ7ZP9NnrHV",
      "metadata": {
        "id": "ZNZ7ZP9NnrHV"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4n3l7Hylkf2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n3l7Hylkf2b",
        "outputId": "5c0e88b9-122b-4cd5-d340-c97cacbe8c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting models/swinv2_backbone.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile models/swinv2_backbone.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from models.common import Conv\n",
        "\n",
        "class SwinV2Backbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-output SwinV2 backbone for YOLOv5.\n",
        "    - Takes 3xHxW RGB images (any size).\n",
        "    - Resizes to expected size for SwinV2.\n",
        "    - Uses timm SwinV2 Tiny.\n",
        "    - Grabs the last stage feature map.\n",
        "    - Projects to c2 channels and resizes output for YOLO.\n",
        "    \"\"\"\n",
        "    def __init__(self, c1, c2, model_name=\"swinv2_tiny_window16_256\", out_index=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Expected input size for the SwinV2 model (256 for swinv2_tiny_window16_256)\n",
        "        self.expected_size = 256\n",
        "\n",
        "        # features_only=True -> returns list of feature maps\n",
        "        self.swin = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            features_only=True,\n",
        "            out_indices=(out_index,)\n",
        "        )\n",
        "        in_ch = self.swin.feature_info.channels()[0]\n",
        "\n",
        "        # Project Swin channels -> c2 channels used by YOLO head\n",
        "        self.proj = Conv(in_ch, c2, k=1, s=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store original size\n",
        "        _, _, orig_h, orig_w = x.shape\n",
        "        \n",
        "        # Resize to expected size if needed (SwinV2 requires fixed input size)\n",
        "        if orig_h != self.expected_size or orig_w != self.expected_size:\n",
        "            x = F.interpolate(x, size=(self.expected_size, self.expected_size), \n",
        "                            mode='bilinear', align_corners=False)\n",
        "        \n",
        "        feats = self.swin(x)  # list with 1 tensor\n",
        "        f = feats[0]          # extract tensor\n",
        "\n",
        "        # NHWC â†’ NCHW\n",
        "        if f.dim() == 4:\n",
        "            f = f.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        out = self.proj(f)    # project to output channels\n",
        "        \n",
        "        # Resize output to match expected stride-32 feature map size\n",
        "        expected_out_h = orig_h // 32\n",
        "        expected_out_w = orig_w // 32\n",
        "        _, _, out_h, out_w = out.shape\n",
        "        \n",
        "        if out_h != expected_out_h or out_w != expected_out_w:\n",
        "            out = F.interpolate(out, size=(expected_out_h, expected_out_w), \n",
        "                              mode='bilinear', align_corners=False)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9rnd_lkskkFz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rnd_lkskkFz",
        "outputId": "2645dfd4-f155-4083-9aba-14039f7ece39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Patched models/yolo.py for SwinV2Backbone\n"
          ]
        }
      ],
      "source": [
        "yolo_path = Path(\"models/yolo.py\")\n",
        "text = yolo_path.read_text()\n",
        "\n",
        "# 1) Add import for SwinV2Backbone\n",
        "if \"SwinV2Backbone\" not in text:\n",
        "    text = text.replace(\n",
        "        \"from models.common import *\",\n",
        "        \"from models.common import *\\nfrom models.swinv2_backbone import SwinV2Backbone\",\n",
        "        1,\n",
        "    )\n",
        "\n",
        "# 2) Patch parse_model to handle SwinV2Backbone\n",
        "snippet = \"\"\"        elif m is Contract:\n",
        "            c2 = ch[f] * args[0] ** 2\n",
        "        elif m is Expand:\n",
        "            c2 = ch[f] // args[0] ** 2\n",
        "        else:\n",
        "            c2 = ch[f]\n",
        "\"\"\"\n",
        "\n",
        "replacement = \"\"\"        elif m is Contract:\n",
        "            c2 = ch[f] * args[0] ** 2\n",
        "        elif m is Expand:\n",
        "            c2 = ch[f] // args[0] ** 2\n",
        "        elif m is SwinV2Backbone:\n",
        "            # args: [c2, model_name, out_index, pretrained]\n",
        "            c1, c2 = ch[f], args[0]\n",
        "            args = [c1, c2, *args[1:]]\n",
        "        else:\n",
        "            c2 = ch[f]\n",
        "\"\"\"\n",
        "\n",
        "if snippet not in text:\n",
        "    raise RuntimeError(\"Expected snippet not found in models/yolo.py. YOLOv5 version mismatch?\")\n",
        "text = text.replace(snippet, replacement)\n",
        "\n",
        "yolo_path.write_text(text)\n",
        "print(\"âœ… Patched models/yolo.py for SwinV2Backbone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jc_-SWyAkpwB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc_-SWyAkpwB",
        "outputId": "0f7061eb-8630-4153-af94-41a7d9ad20b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Created models/yolov5m_swinv2.yaml\n"
          ]
        }
      ],
      "source": [
        "swin_yaml = r\"\"\"\n",
        "# YOLOv5 + SwinV2 Tiny backbone (single-scale detection)\n",
        "nc: 3\n",
        "\n",
        "# Single scale anchors - only one set for single-scale detection\n",
        "anchors:\n",
        "  - [30,61, 62,45, 59,119]\n",
        "\n",
        "depth_multiple: 1.0\n",
        "width_multiple: 1.0\n",
        "\n",
        "backbone:\n",
        "  # args: [c2, model_name, out_index, pretrained]\n",
        "  # c1 is automatically set from previous layer channels\n",
        "  - [-1, 1, SwinV2Backbone, [256, 'swinv2_tiny_window16_256', 3, true]]\n",
        "\n",
        "head:\n",
        "  # C3 args: [c2] - n is automatically handled by the repeat count (2nd value)\n",
        "  - [0, 1, C3, [256]]\n",
        "  # Single-scale Detect: input from layer 1 only, uses single anchor set\n",
        "  - [[1], 1, Detect, [nc, anchors]]\n",
        "\"\"\"\n",
        "\n",
        "Path(\"models/yolov5m_swinv2.yaml\").write_text(swin_yaml)\n",
        "print(\"âœ… Created models/yolov5m_swinv2.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "AHLbvL4WktFD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHLbvL4WktFD",
        "outputId": "f1754f69-ebf9-4a7e-9665-6ad03c287405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… data/vehicles.yaml written\n"
          ]
        }
      ],
      "source": [
        "dataset_yaml = f\"\"\"\n",
        "train: /content/data/AAU_YOLO/images/train\n",
        "val: /content/data/AAU_YOLO/images/val\n",
        "nc: 3\n",
        "names: ['car', 'truck', 'bus']\n",
        "\"\"\"\n",
        "\n",
        "Path(\"data/vehicles.yaml\").write_text(dataset_yaml)\n",
        "print(\"âœ… data/vehicles.yaml written\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PTOvQnnmqxwp",
      "metadata": {
        "id": "PTOvQnnmqxwp"
      },
      "source": [
        "## Process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "Tg7Ljl7epaD3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7Ljl7epaD3",
        "outputId": "a7d74ed1-4562-4272-896c-cb3996e9f4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ” Output folders created: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/AAU_YOLO\n"
          ]
        }
      ],
      "source": [
        "json_path = DATA_ROOT/\"aauRainSnow-rgb.json\"\n",
        "\n",
        "output_root = PROJECT_ROOT/\"data/AAU_YOLO\"\n",
        "output_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create folder structure\n",
        "for folder in [\"images/train\",\"images/val\",\"labels/train\",\"labels/val\"]:\n",
        "    (output_root/folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ” Output folders created:\", output_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ugtU93JbrgT-",
      "metadata": {
        "id": "ugtU93JbrgT-"
      },
      "source": [
        "## Load JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "68ZeFkkOrdQW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ZeFkkOrdQW",
        "outputId": "1260a314-ded1-4ddf-b2b8-d5ce60b21a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JSON: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/training_data_vehicles_in_rain/aauRainSnow-rgb.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading JSON:\", json_path)\n",
        "with open(json_path) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "images = {im[\"id\"]: im for im in coco[\"images\"]}\n",
        "\n",
        "# Map AAU classes â†’ YOLO classes\n",
        "VEHICLE_MAP = {3:0, 6:1, 8:2}  # carâ†’0, truckâ†’1, busâ†’2\n",
        "\n",
        "# Collect annotations per image\n",
        "anns = {}\n",
        "for ann in coco[\"annotations\"]:\n",
        "    if ann[\"category_id\"] in VEHICLE_MAP:\n",
        "        anns.setdefault(ann[\"image_id\"], []).append(ann)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "txh17-bzrmC7",
      "metadata": {
        "id": "txh17-bzrmC7"
      },
      "source": [
        "## Train/Val Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "G-IRKFGnpqmv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-IRKFGnpqmv",
        "outputId": "2e8c92dd-f70f-493d-bcda-bc018f8f6078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 1597, Val images: 400\n"
          ]
        }
      ],
      "source": [
        "image_ids = list(anns.keys())\n",
        "np.random.shuffle(image_ids)\n",
        "\n",
        "split = int(0.8 * len(image_ids))\n",
        "train_ids = image_ids[:split]\n",
        "val_ids = image_ids[split:]\n",
        "\n",
        "print(f\"Train images: {len(train_ids)}, Val images: {len(val_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5qtmL54Jptek",
      "metadata": {
        "id": "5qtmL54Jptek"
      },
      "outputs": [],
      "source": [
        "def coco_to_yolo(img_id, img_dir, lbl_dir):\n",
        "    info = images[img_id]\n",
        "\n",
        "    src_img = DATA_ROOT / info[\"file_name\"]\n",
        "    fname = Path(info[\"file_name\"]).name\n",
        "\n",
        "    # Copy image\n",
        "    dst_img = img_dir / fname\n",
        "    shutil.copy2(src_img, dst_img)\n",
        "\n",
        "    w, h = info[\"width\"], info[\"height\"]\n",
        "\n",
        "    # Create YOLO label\n",
        "    dst_lbl = lbl_dir / (fname.replace(\".png\", \".txt\").replace(\".jpg\",\".txt\"))\n",
        "\n",
        "    lines = []\n",
        "    for ann in anns[img_id]:\n",
        "        cls = VEHICLE_MAP[ann[\"category_id\"]]\n",
        "        x, y, bw, bh = ann[\"bbox\"]\n",
        "\n",
        "        xc = (x + bw/2) / w\n",
        "        yc = (y + bh/2) / h\n",
        "        bw /= w\n",
        "        bh /= h\n",
        "\n",
        "        lines.append(f\"{cls} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\")\n",
        "\n",
        "    dst_lbl.write_text(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8d2GtsDdpwYv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d2GtsDdpwYv",
        "outputId": "9a6e7322-8a58-4e6e-b9ed-1569cce0b809"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1597/1597 [48:49<00:00,  1.83s/it]\n",
            "Processing val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:38<00:00,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ” Dataset conversion completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def process(ids, split):\n",
        "    img_dir = output_root / f\"images/{split}\"\n",
        "    lbl_dir = output_root / f\"labels/{split}\"\n",
        "\n",
        "    for img_id in tqdm(ids, desc=f\"Processing {split}\"):\n",
        "        coco_to_yolo(img_id, img_dir, lbl_dir)\n",
        "\n",
        "process(train_ids, \"train\")\n",
        "process(val_ids, \"val\")\n",
        "\n",
        "print(\"âœ” Dataset conversion completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "UPgiS20Ypzsx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPgiS20Ypzsx",
        "outputId": "8121fe6f-6b3d-40dd-cc96-d18fcfb0353b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ” vehicles.yaml created\n"
          ]
        }
      ],
      "source": [
        "dataset_yaml = f\"\"\"\n",
        "train: {output_root}/images/train\n",
        "val: {output_root}/images/val\n",
        "\n",
        "nc: 3\n",
        "names: ['car','truck','bus']\n",
        "\"\"\"\n",
        "\n",
        "(Path(\"/content/yolov5/data/vehicles.yaml\")).write_text(dataset_yaml)\n",
        "print(\"âœ” vehicles.yaml created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8anEE4zopnsg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8anEE4zopnsg",
        "outputId": "112353a9-88cb-4871-cd90-ee6cfb649419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JSON: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/training_data_vehicles_in_rain/aauRainSnow-rgb.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading JSON:\", json_path)\n",
        "with open(json_path) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "images = {im[\"id\"]: im for im in coco[\"images\"]}\n",
        "\n",
        "# Map AAU classes â†’ YOLO classes\n",
        "VEHICLE_MAP = {3:0, 6:1, 8:2}  # carâ†’0, truckâ†’1, busâ†’2\n",
        "\n",
        "# Collect annotations per image\n",
        "anns = {}\n",
        "for ann in coco[\"annotations\"]:\n",
        "    if ann[\"category_id\"] in VEHICLE_MAP:\n",
        "        anns.setdefault(ann[\"image_id\"], []).append(ann)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "giGkf43P6lX8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giGkf43P6lX8",
        "outputId": "d0eb0358-20eb-4cbe-bfad-0fa028435c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ” Successfully inserted SwinV2Backbone import before models.common.\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/yolov5/models/yolo.py\"\n",
        "insert_line = \"from models.swinv2_backbone import SwinV2Backbone\\n\"\n",
        "\n",
        "# Read file\n",
        "with open(file_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Only insert if not already present\n",
        "if insert_line not in \"\".join(lines):\n",
        "\n",
        "    new_lines = []\n",
        "    inserted = False\n",
        "\n",
        "    for line in lines:\n",
        "        # Before the common import, insert our Swin import\n",
        "        if line.startswith(\"from models.common import\"):\n",
        "            new_lines.append(insert_line)   # INSERT HERE\n",
        "            inserted = True\n",
        "\n",
        "        new_lines.append(line)\n",
        "\n",
        "    if inserted:\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.writelines(new_lines)\n",
        "        print(\"âœ” Successfully inserted SwinV2Backbone import before models.common.\")\n",
        "    else:\n",
        "        print(\"Could not find 'from models.common import (' in yolo.py.\")\n",
        "else:\n",
        "    print(f\"âœ” SwinV2Backbone import already present â€” no changes made.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "JbL6TVoW9YLS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbL6TVoW9YLS",
        "outputId": "e7e094c5-3c75-48bb-a72d-19eee8498729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from models.swinv2_backbone import SwinV2Backbone\n",
            "\n",
            "        elif m is SwinV2Backbone:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/yolov5/models/yolo.py\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "  if \"SwinV2Backbone\" in line:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "qswSRTx4kwMh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qswSRTx4kwMh",
        "outputId": "537bf760-4217-4af4-d6cb-295a76d7d4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2025-12-13 04:48:00.862698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765601280.884170   36342 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765601280.890674   36342 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765601280.907167   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765601280.907194   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765601280.907197   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765601280.907200   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=models/yolov5m_swinv2.yaml, data=data/vehicles.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=15, batch_size=16, imgsz=256, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=YOLO_SwinV2, name=rainsnow_swinv2, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-450-g781b9d57 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir YOLO_SwinV2', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/train.py\", line 987, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/train.py\", line 688, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov5/train.py\", line 225, in train\n",
            "    model = Model(cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov5/models/yolo.py\", line 242, in __init__\n",
            "    self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov5/models/yolo.py\", line 456, in parse_model\n",
            "    m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n",
            "                                                                    ^^^^^^^^\n",
            "  File \"/content/yolov5/models/swinv2_backbone.py\", line 18, in __init__\n",
            "    self.swin = timm.create_model(\n",
            "                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py\", line 108, in create_model\n",
            "    model_source, model_id = parse_model_name(model_name)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py\", line 20, in parse_model_name\n",
            "    if model_name.startswith('hf_hub'):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'int' object has no attribute 'startswith'\n"
          ]
        }
      ],
      "source": [
        "%cd /content/yolov5\n",
        "\n",
        "!python train.py \\\n",
        "  --img 256 \\\n",
        "  --batch 16 \\\n",
        "  --epochs 15 \\\n",
        "  --data data/vehicles.yaml \\\n",
        "  --cfg models/yolov5m_swinv2.yaml \\\n",
        "  --weights '' \\\n",
        "  --project YOLO_SwinV2 \\\n",
        "  --name rainsnow_swinv2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a1cde1",
      "metadata": {},
      "source": [
        "## Video Inference with Trained Model\n",
        "\n",
        "Run the trained YOLO-SwinV2 model on the highway video to detect vehicles. The trained weights from the best epoch will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a19a025",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VIDEO INFERENCE WITH TRAINED MODEL\n",
        "# ============================================================================\n",
        "\n",
        "# Path to trained weights (update if your run folder name is different)\n",
        "WEIGHTS_PATH = \"YOLO_SwinV2/rainsnow_swinv214/weights/best.pt\"\n",
        "\n",
        "# Check if weights exist, find the latest if not\n",
        "import glob\n",
        "if not os.path.exists(WEIGHTS_PATH):\n",
        "    # Find the latest training run\n",
        "    runs = sorted(glob.glob(\"YOLO_SwinV2/rainsnow_swinv2*/weights/best.pt\"))\n",
        "    if runs:\n",
        "        WEIGHTS_PATH = runs[-1]\n",
        "        print(f\"Using weights: {WEIGHTS_PATH}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No trained weights found! Run training first.\")\n",
        "\n",
        "# Set output directory\n",
        "if IN_COLAB:\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/DL/enhanced_vehicle_detection/outputs/YOLO_SwinV2'\n",
        "else:\n",
        "    OUTPUT_DIR = '../outputs/YOLO_SwinV2'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'visualizations'), exist_ok=True)\n",
        "\n",
        "# Run YOLOv5 detect.py on the video\n",
        "output_detect_dir = os.path.join(OUTPUT_DIR, 'detect_output')\n",
        "\n",
        "!python detect.py \\\n",
        "    --weights {WEIGHTS_PATH} \\\n",
        "    --source {VIDEO_PATH} \\\n",
        "    --img 256 \\\n",
        "    --conf-thres 0.25 \\\n",
        "    --iou-thres 0.45 \\\n",
        "    --project {OUTPUT_DIR} \\\n",
        "    --name detect_output \\\n",
        "    --exist-ok \\\n",
        "    --save-txt \\\n",
        "    --save-conf\n",
        "\n",
        "print(f\"\\nDetection complete!\")\n",
        "print(f\"Results saved to: {output_detect_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "546b6204",
      "metadata": {},
      "source": [
        "## Display Detection Video\n",
        "\n",
        "Convert the output video to MP4 format and display it in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2396fc9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check what files were created\n",
        "import os\n",
        "import glob\n",
        "\n",
        "detect_output_dir = os.path.join(OUTPUT_DIR, 'detect_output')\n",
        "print(f\"Checking directory: {detect_output_dir}\")\n",
        "\n",
        "if os.path.exists(detect_output_dir):\n",
        "    all_files = os.listdir(detect_output_dir)\n",
        "    print(f\"\\nFiles in detect_output:\")\n",
        "    for f in all_files:\n",
        "        filepath = os.path.join(detect_output_dir, f)\n",
        "        size = os.path.getsize(filepath) if os.path.isfile(filepath) else 0\n",
        "        print(f\"  - {f} ({size / 1024:.1f} KB)\")\n",
        "else:\n",
        "    print(\"detect_output directory not found!\")\n",
        "\n",
        "# Also check the main output dir\n",
        "print(f\"\\nFiles in OUTPUT_DIR ({OUTPUT_DIR}):\")\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    for f in os.listdir(OUTPUT_DIR):\n",
        "        filepath = os.path.join(OUTPUT_DIR, f)\n",
        "        if os.path.isfile(filepath):\n",
        "            size = os.path.getsize(filepath)\n",
        "            print(f\"  - {f} ({size / 1024:.1f} KB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbddba9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detection video - use the file from detect_output\n",
        "detect_video = os.path.join(OUTPUT_DIR, 'detect_output', 'rainy_highway_video.mp4')\n",
        "\n",
        "if os.path.exists(detect_video):\n",
        "    file_size_mb = os.path.getsize(detect_video) / 1024 / 1024\n",
        "    print(f\"âœ… Detection video found: {detect_video}\")\n",
        "    print(f\"   Size: {file_size_mb:.1f} MB\")\n",
        "    \n",
        "    if file_size_mb > 25:\n",
        "        # For large files, try a compressed version\n",
        "        compressed_video = os.path.join(OUTPUT_DIR, 'detection_compressed.mp4')\n",
        "        print(\"\\nCreating compressed version for display...\")\n",
        "        os.system(f'ffmpeg -y -i \"{detect_video}\" -vcodec libx264 -crf 28 -preset fast -vf scale=640:-2 \"{compressed_video}\" -loglevel error')\n",
        "        \n",
        "        if os.path.exists(compressed_video) and os.path.getsize(compressed_video) > 1000:\n",
        "            print(f\"Compressed: {os.path.getsize(compressed_video)/1024/1024:.1f} MB\")\n",
        "            video_to_display = compressed_video\n",
        "        else:\n",
        "            video_to_display = detect_video\n",
        "    else:\n",
        "        video_to_display = detect_video\n",
        "    \n",
        "    print(\"\\nðŸŽ¬ YOLO-SwinV2 Vehicle Detection Result:\\n\")\n",
        "    video_data = open(video_to_display, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "    display(HTML(f'''\n",
        "        <video width=\"800\" controls autoplay muted>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "            Your browser does not support the video tag.\n",
        "        </video>\n",
        "    '''))\n",
        "else:\n",
        "    print(f\"Video not found at: {detect_video}\")\n",
        "    # List what's available\n",
        "    print(\"\\nAvailable files in OUTPUT_DIR:\")\n",
        "    for f in os.listdir(OUTPUT_DIR):\n",
        "        print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcc619b",
      "metadata": {},
      "source": [
        "## Enhanced Detection with Tracking and Optical Flow\n",
        "\n",
        "Apply the same post-processing pipeline as YOLO-V5m:\n",
        "- **IoU-based tracking**: Match detections across frames using Intersection over Union\n",
        "- **Farneback Optical Flow**: Compute dense motion between frames for smoother tracking\n",
        "- **Track management**: Create, update, and remove tracks with unique IDs\n",
        "- **Visualization**: Draw bounding boxes with track IDs and motion trails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a122828",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ENHANCED DETECTION WITH TRACKING (Same pipeline as YOLO-V5m)\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/yolov5')\n",
        "from torchvision.ops import nms\n",
        "\n",
        "# ============================================================================\n",
        "# DETECTION QUALITY SETTINGS - Adjust these to improve results\n",
        "# ============================================================================\n",
        "FPS = 25\n",
        "CONF_THRESH = 0.40        # Higher = fewer but more confident detections (was 0.25)\n",
        "NMS_IOU_THRESH = 0.30     # Lower = more aggressive overlap removal (was 0.45)\n",
        "IOU_MATCH_THRESH = 0.3    # For tracking between frames\n",
        "MIN_BOX_AREA = 500        # Minimum bounding box area in pixels (filters tiny boxes)\n",
        "MAX_DETECTIONS = 20       # Maximum detections per frame (prevents clutter)\n",
        "\n",
        "VEHICLE_CLASSES = {0: 'car', 1: 'truck', 2: 'bus'}\n",
        "\n",
        "print(\"Detection Quality Settings:\")\n",
        "print(f\"  - Confidence threshold: {CONF_THRESH} (higher = stricter)\")\n",
        "print(f\"  - NMS IoU threshold: {NMS_IOU_THRESH} (lower = less overlap)\")\n",
        "print(f\"  - Min box area: {MIN_BOX_AREA} pixels\")\n",
        "print(f\"  - Max detections/frame: {MAX_DETECTIONS}\")\n",
        "\n",
        "# Load the trained model\n",
        "print(\"\\nLoading trained YOLO-SwinV2 model...\")\n",
        "model = torch.hub.load('/content/yolov5', 'custom', path=WEIGHTS_PATH, source='local')\n",
        "model.conf = CONF_THRESH\n",
        "model.iou = NMS_IOU_THRESH\n",
        "print(f\"Model loaded from: {WEIGHTS_PATH}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Helper Functions (same as YOLO-V5m)\n",
        "# ============================================================================\n",
        "\n",
        "def detect_vehicles(frame):\n",
        "    \"\"\"Run inference on a frame and return filtered detections.\"\"\"\n",
        "    results = model(frame[:, :, ::-1])  # BGR to RGB\n",
        "    \n",
        "    raw_detections = results.xyxy[0].cpu()\n",
        "    \n",
        "    if len(raw_detections) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Extract boxes, scores, and classes\n",
        "    boxes = raw_detections[:, :4]\n",
        "    scores = raw_detections[:, 4]\n",
        "    classes = raw_detections[:, 5].int()\n",
        "    \n",
        "    # Filter by vehicle classes\n",
        "    vehicle_mask = torch.zeros(len(classes), dtype=torch.bool)\n",
        "    for cls_id in VEHICLE_CLASSES.keys():\n",
        "        vehicle_mask |= (classes == cls_id)\n",
        "    \n",
        "    if not vehicle_mask.any():\n",
        "        return []\n",
        "    \n",
        "    boxes = boxes[vehicle_mask]\n",
        "    scores = scores[vehicle_mask]\n",
        "    classes = classes[vehicle_mask]\n",
        "    \n",
        "    # Filter by minimum box area\n",
        "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "    area_mask = areas >= MIN_BOX_AREA\n",
        "    boxes = boxes[area_mask]\n",
        "    scores = scores[area_mask]\n",
        "    classes = classes[area_mask]\n",
        "    \n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Apply additional NMS to remove remaining overlaps\n",
        "    keep_indices = nms(boxes, scores, NMS_IOU_THRESH)\n",
        "    \n",
        "    # Limit max detections (keep highest confidence)\n",
        "    if len(keep_indices) > MAX_DETECTIONS:\n",
        "        # Sort by score and keep top N\n",
        "        sorted_idx = torch.argsort(scores[keep_indices], descending=True)\n",
        "        keep_indices = keep_indices[sorted_idx[:MAX_DETECTIONS]]\n",
        "    \n",
        "    detections = []\n",
        "    for idx in keep_indices:\n",
        "        x1, y1, x2, y2 = boxes[idx].numpy().astype(int)\n",
        "        detections.append({\n",
        "            \"bbox\": [x1, y1, x2, y2],\n",
        "            \"conf\": float(scores[idx]),\n",
        "            \"class\": VEHICLE_CLASSES[int(classes[idx])]\n",
        "        })\n",
        "    \n",
        "    return detections\n",
        "\n",
        "def iou_xyxy(boxA, boxB):\n",
        "    \"\"\"Compute IoU between two boxes in [x1, y1, x2, y2] format.\"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    \n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    \n",
        "    if interArea == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    \n",
        "    return interArea / float(areaA + areaB - interArea)\n",
        "\n",
        "def update_tracks(detections, flow, frame_index):\n",
        "    \"\"\"Update tracks based on detections and optical flow.\"\"\"\n",
        "    global tracks, next_track_id\n",
        "    \n",
        "    unmatched_tracks = set(tracks.keys())\n",
        "    det_to_track = {}\n",
        "    \n",
        "    # IoU-based matching\n",
        "    for det_idx, det in enumerate(detections):\n",
        "        box_det = det[\"bbox\"]\n",
        "        best_iou = 0\n",
        "        best_track = None\n",
        "        \n",
        "        for tid in unmatched_tracks:\n",
        "            box_tr = tracks[tid][\"bbox\"]\n",
        "            iou_val = iou_xyxy(box_det, box_tr)\n",
        "            if iou_val > best_iou:\n",
        "                best_iou = iou_val\n",
        "                best_track = tid\n",
        "        \n",
        "        if best_iou > IOU_MATCH_THRESH:\n",
        "            det_to_track[det_idx] = best_track\n",
        "            unmatched_tracks.remove(best_track)\n",
        "    \n",
        "    # Update matched tracks\n",
        "    for det_idx, track_id in det_to_track.items():\n",
        "        x1, y1, x2, y2 = detections[det_idx][\"bbox\"]\n",
        "        cx = (x1 + x2) // 2\n",
        "        cy = (y1 + y2) // 2\n",
        "        \n",
        "        tracks[track_id][\"bbox\"] = (x1, y1, x2, y2)\n",
        "        tracks[track_id][\"conf\"] = detections[det_idx][\"conf\"]\n",
        "        tracks[track_id][\"class\"] = detections[det_idx][\"class\"]\n",
        "        tracks[track_id][\"trace\"].append((cx, cy))\n",
        "        tracks[track_id][\"last_seen\"] = frame_index\n",
        "    \n",
        "    # Create new tracks for unmatched detections\n",
        "    for det_idx, det in enumerate(detections):\n",
        "        if det_idx in det_to_track:\n",
        "            continue\n",
        "        \n",
        "        x1, y1, x2, y2 = det[\"bbox\"]\n",
        "        cx = (x1 + x2) // 2\n",
        "        cy = (y1 + y2) // 2\n",
        "        \n",
        "        tracks[next_track_id] = {\n",
        "            \"bbox\": (x1, y1, x2, y2),\n",
        "            \"conf\": det[\"conf\"],\n",
        "            \"class\": det[\"class\"],\n",
        "            \"trace\": [(cx, cy)],\n",
        "            \"last_seen\": frame_index\n",
        "        }\n",
        "        next_track_id += 1\n",
        "    \n",
        "    # Remove tracks that haven't been seen recently\n",
        "    max_missing_frames = FPS\n",
        "    tracks_to_remove = [tid for tid, tr in tracks.items() \n",
        "                       if frame_index - tr[\"last_seen\"] > max_missing_frames]\n",
        "    for tid in tracks_to_remove:\n",
        "        del tracks[tid]\n",
        "\n",
        "# ============================================================================\n",
        "# Process Video with Tracking\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\nProcessing video with tracking: {VIDEO_PATH}\")\n",
        "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
        "\n",
        "# Video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS)) or FPS\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f\"Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
        "\n",
        "# Output paths\n",
        "output_avi = os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_with_tracking.avi')\n",
        "output_mp4 = os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_with_tracking.mp4')\n",
        "\n",
        "# Video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(output_avi, fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize tracking\n",
        "tracks = {}\n",
        "next_track_id = 0\n",
        "\n",
        "# Metrics collection\n",
        "metrics = {\n",
        "    \"frame_indices\": [],\n",
        "    \"detections_per_frame\": [],\n",
        "    \"confidence_scores\": [],\n",
        "    \"class_counts\": {\"car\": [], \"truck\": [], \"bus\": []},\n",
        "}\n",
        "\n",
        "# Read first frame for optical flow\n",
        "ret, frame = cap.read()\n",
        "if not ret:\n",
        "    raise RuntimeError(\"Couldn't read the video\")\n",
        "\n",
        "old_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "frame_index = 0\n",
        "pbar = tqdm(total=total_frames, desc='Processing with tracking')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    frame_index += 1\n",
        "    \n",
        "    # Detect vehicles\n",
        "    dets = detect_vehicles(frame)\n",
        "    \n",
        "    # Collect metrics\n",
        "    metrics[\"frame_indices\"].append(frame_index)\n",
        "    metrics[\"detections_per_frame\"].append(len(dets))\n",
        "    \n",
        "    class_count = {\"car\": 0, \"truck\": 0, \"bus\": 0}\n",
        "    for det in dets:\n",
        "        metrics[\"confidence_scores\"].append(det[\"conf\"])\n",
        "        class_count[det[\"class\"]] += 1\n",
        "    \n",
        "    for cls in class_count:\n",
        "        metrics[\"class_counts\"][cls].append(class_count[cls])\n",
        "    \n",
        "    # Compute optical flow\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(\n",
        "        old_gray, frame_gray, None,\n",
        "        0.5,   # pyr_scale\n",
        "        3,     # levels\n",
        "        15,    # winsize\n",
        "        3,     # iterations\n",
        "        5,     # poly_n\n",
        "        1.2,   # poly_sigma\n",
        "        0      # flags\n",
        "    )\n",
        "    \n",
        "    # Update tracks\n",
        "    update_tracks(dets, flow, frame_index)\n",
        "    \n",
        "    # Class-specific colors (BGR format)\n",
        "    CLASS_COLORS = {\n",
        "        'car': (0, 255, 0),      # Green\n",
        "        'truck': (255, 165, 0),   # Orange  \n",
        "        'bus': (255, 0, 255),     # Magenta\n",
        "    }\n",
        "    \n",
        "    # Draw tracks\n",
        "    for track_id, track in tracks.items():\n",
        "        x1, y1, x2, y2 = track[\"bbox\"]\n",
        "        conf = track[\"conf\"]\n",
        "        cls_name = track[\"class\"]\n",
        "        \n",
        "        color = CLASS_COLORS.get(cls_name, (0, 255, 0))\n",
        "        \n",
        "        # Draw bounding box with thicker line\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
        "        \n",
        "        # Draw label background for better visibility\n",
        "        label = f\"ID:{track_id} {cls_name} {conf:.2f}\"\n",
        "        (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "        cv2.rectangle(frame, (x1, y1 - text_h - 10), (x1 + text_w + 4, y1), color, -1)\n",
        "        cv2.putText(frame, label, (x1 + 2, y1 - 5), \n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        \n",
        "        # Draw tracking trail (last 30 points only)\n",
        "        trace = track[\"trace\"][-30:]  # Limit trail length\n",
        "        if len(trace) > 1:\n",
        "            pts = np.array(trace, dtype=np.int32).reshape(-1, 1, 2)\n",
        "            cv2.polylines(frame, [pts], False, (0, 255, 255), 2)\n",
        "    \n",
        "    out.write(frame)\n",
        "    old_gray = frame_gray.copy()\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Convert to MP4\n",
        "print(\"\\nConverting to MP4...\")\n",
        "os.system(f'ffmpeg -y -i \"{output_avi}\" -vcodec libx264 -crf 23 -pix_fmt yuv420p \"{output_mp4}\" -loglevel error')\n",
        "\n",
        "if os.path.exists(output_mp4) and os.path.getsize(output_mp4) > 1000:\n",
        "    print(f\"âœ… Video saved to: {output_mp4}\")\n",
        "    os.remove(output_avi)\n",
        "else:\n",
        "    print(f\"âš ï¸ Saved as AVI: {output_avi}\")\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"           YOLO-SwinV2 DETECTION SUMMARY (WITH TRACKING)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total frames processed: {frame_index}\")\n",
        "print(f\"Total detections: {sum(metrics['detections_per_frame'])}\")\n",
        "print(f\"Avg detections/frame: {np.mean(metrics['detections_per_frame']):.2f}\")\n",
        "if metrics[\"confidence_scores\"]:\n",
        "    print(f\"Mean confidence: {np.mean(metrics['confidence_scores']):.4f}\")\n",
        "print(f\"Total cars: {sum(metrics['class_counts']['car'])}\")\n",
        "print(f\"Total trucks: {sum(metrics['class_counts']['truck'])}\")\n",
        "print(f\"Total buses: {sum(metrics['class_counts']['bus'])}\")\n",
        "print(f\"Unique tracks created: {next_track_id}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b14661d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the video with tracking\n",
        "print(\"ðŸŽ¬ YOLO-SwinV2 Vehicle Detection with Tracking:\\n\")\n",
        "\n",
        "if os.path.exists(output_mp4):\n",
        "    file_size_mb = os.path.getsize(output_mp4) / 1024 / 1024\n",
        "    print(f\"Video: {output_mp4} ({file_size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Compress for display if large\n",
        "    if file_size_mb > 20:\n",
        "        compressed = os.path.join(OUTPUT_DIR, 'tracking_compressed.mp4')\n",
        "        os.system(f'ffmpeg -y -i \"{output_mp4}\" -vcodec libx264 -crf 28 -preset fast -vf scale=640:-2 \"{compressed}\" -loglevel error')\n",
        "        if os.path.exists(compressed):\n",
        "            video_to_show = compressed\n",
        "            print(f\"Compressed for display: {os.path.getsize(compressed)/1024/1024:.1f} MB\")\n",
        "        else:\n",
        "            video_to_show = output_mp4\n",
        "    else:\n",
        "        video_to_show = output_mp4\n",
        "    \n",
        "    video_data = open(video_to_show, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "    display(HTML(f'''\n",
        "        <video width=\"800\" controls autoplay muted>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    '''))\n",
        "else:\n",
        "    print(f\"Video not found: {output_mp4}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd78ccb",
      "metadata": {},
      "source": [
        "## Metrics Visualization and Comparison with YOLO-V5m\n",
        "\n",
        "Visualize inference metrics and compare with the pre-trained YOLO-V5m baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56132b04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METRICS VISUALIZATION AND COMPARISON\n",
        "# ============================================================================\n",
        "\n",
        "viz_dir = os.path.join(OUTPUT_DIR, 'visualizations')\n",
        "os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "# Load YOLO-V5m metrics for comparison\n",
        "yolov5m_metrics_path = PROJECT_ROOT / 'outputs' / 'YOLO_V5m' / 'YOLO_V5m_metrics.json'\n",
        "yolov5m_metrics = None\n",
        "\n",
        "if os.path.exists(yolov5m_metrics_path):\n",
        "    with open(yolov5m_metrics_path) as f:\n",
        "        yolov5m_metrics = json.load(f)\n",
        "    print(f\"âœ… Loaded YOLO-V5m metrics from: {yolov5m_metrics_path}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ YOLO-V5m metrics not found at: {yolov5m_metrics_path}\")\n",
        "    print(\"   Run YOLO_V5m.ipynb first for comparison.\")\n",
        "\n",
        "# ============================================================================\n",
        "# Individual Metric Plots\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Detections per Frame\n",
        "fig1, ax1 = plt.subplots(figsize=(12, 5))\n",
        "ax1.plot(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], \n",
        "         color='#E63946', linewidth=1.5, label='YOLO-SwinV2')\n",
        "ax1.fill_between(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], \n",
        "                 alpha=0.3, color='#E63946')\n",
        "if yolov5m_metrics:\n",
        "    ax1.plot(yolov5m_metrics['per_frame_data']['frame_indices'], \n",
        "             yolov5m_metrics['per_frame_data']['detections_per_frame'],\n",
        "             color='#2E86AB', linewidth=1.5, alpha=0.7, label='YOLO-V5m')\n",
        "ax1.set_xlabel('Frame Index')\n",
        "ax1.set_ylabel('Number of Detections')\n",
        "ax1.set_title('Detections per Frame Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(viz_dir, 'detections_per_frame_comparison.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# 2. Confidence Score Distribution\n",
        "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "if metrics[\"confidence_scores\"]:\n",
        "    ax2.hist(metrics[\"confidence_scores\"], bins=30, color='#E63946', \n",
        "             edgecolor='white', alpha=0.7, label='YOLO-SwinV2')\n",
        "    ax2.axvline(np.mean(metrics[\"confidence_scores\"]), color='#E63946', \n",
        "                linestyle='--', linewidth=2, \n",
        "                label=f'SwinV2 Mean: {np.mean(metrics[\"confidence_scores\"]):.3f}')\n",
        "\n",
        "if yolov5m_metrics and yolov5m_metrics['summary']['mean_confidence'] > 0:\n",
        "    ax2.axvline(yolov5m_metrics['summary']['mean_confidence'], color='#2E86AB', \n",
        "                linestyle='--', linewidth=2,\n",
        "                label=f'V5m Mean: {yolov5m_metrics[\"summary\"][\"mean_confidence\"]:.3f}')\n",
        "ax2.set_xlabel('Confidence Score')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Confidence Score Distribution')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(viz_dir, 'confidence_distribution.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# 3. Class Distribution Over Time\n",
        "fig3, ax3 = plt.subplots(figsize=(12, 5))\n",
        "frames = metrics[\"frame_indices\"]\n",
        "ax3.stackplot(frames, \n",
        "              metrics[\"class_counts\"][\"car\"],\n",
        "              metrics[\"class_counts\"][\"truck\"],\n",
        "              metrics[\"class_counts\"][\"bus\"],\n",
        "              labels=['Car', 'Truck', 'Bus'],\n",
        "              colors=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)\n",
        "ax3.legend(loc='upper right')\n",
        "ax3.set_xlabel('Frame Index')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.set_title('YOLO-SwinV2: Detection Count by Class')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(viz_dir, 'class_distribution_over_time.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# 4. Model Comparison Summary\n",
        "if yolov5m_metrics:\n",
        "    fig4, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Total Detections\n",
        "    models = ['YOLO-V5m', 'YOLO-SwinV2']\n",
        "    total_dets = [yolov5m_metrics['summary']['total_detections'],\n",
        "                  sum(metrics['detections_per_frame'])]\n",
        "    colors = ['#2E86AB', '#E63946']\n",
        "    axes[0].bar(models, total_dets, color=colors)\n",
        "    axes[0].set_ylabel('Total Detections')\n",
        "    axes[0].set_title('Total Detections Comparison')\n",
        "    for i, v in enumerate(total_dets):\n",
        "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "    \n",
        "    # Mean Confidence\n",
        "    mean_confs = [yolov5m_metrics['summary']['mean_confidence'],\n",
        "                  np.mean(metrics['confidence_scores']) if metrics['confidence_scores'] else 0]\n",
        "    axes[1].bar(models, mean_confs, color=colors)\n",
        "    axes[1].set_ylabel('Mean Confidence')\n",
        "    axes[1].set_title('Mean Confidence Comparison')\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    for i, v in enumerate(mean_confs):\n",
        "        axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Class Distribution\n",
        "    x = np.arange(3)\n",
        "    width = 0.35\n",
        "    v5m_classes = [yolov5m_metrics['summary']['total_cars'],\n",
        "                   yolov5m_metrics['summary']['total_trucks'],\n",
        "                   yolov5m_metrics['summary']['total_buses']]\n",
        "    swin_classes = [sum(metrics['class_counts']['car']),\n",
        "                    sum(metrics['class_counts']['truck']),\n",
        "                    sum(metrics['class_counts']['bus'])]\n",
        "    axes[2].bar(x - width/2, v5m_classes, width, label='YOLO-V5m', color='#2E86AB')\n",
        "    axes[2].bar(x + width/2, swin_classes, width, label='YOLO-SwinV2', color='#E63946')\n",
        "    axes[2].set_xticks(x)\n",
        "    axes[2].set_xticklabels(['Car', 'Truck', 'Bus'])\n",
        "    axes[2].set_ylabel('Count')\n",
        "    axes[2].set_title('Detection Count by Class')\n",
        "    axes[2].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(viz_dir, 'model_comparison.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# Print Comparison Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"                    MODEL COMPARISON SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"{'Metric':<30} {'YOLO-V5m':>15} {'YOLO-SwinV2':>15}\")\n",
        "print(f\"{'-'*70}\")\n",
        "\n",
        "if yolov5m_metrics:\n",
        "    print(f\"{'Total Frames':<30} {yolov5m_metrics['summary']['total_frames']:>15} {len(metrics['frame_indices']):>15}\")\n",
        "    print(f\"{'Total Detections':<30} {yolov5m_metrics['summary']['total_detections']:>15} {sum(metrics['detections_per_frame']):>15}\")\n",
        "    print(f\"{'Avg Detections/Frame':<30} {yolov5m_metrics['summary']['avg_detections_per_frame']:>15.2f} {np.mean(metrics['detections_per_frame']):>15.2f}\")\n",
        "    print(f\"{'Mean Confidence':<30} {yolov5m_metrics['summary']['mean_confidence']:>15.4f} {np.mean(metrics['confidence_scores']) if metrics['confidence_scores'] else 0:>15.4f}\")\n",
        "    print(f\"{'Total Cars':<30} {yolov5m_metrics['summary']['total_cars']:>15} {sum(metrics['class_counts']['car']):>15}\")\n",
        "    print(f\"{'Total Trucks':<30} {yolov5m_metrics['summary']['total_trucks']:>15} {sum(metrics['class_counts']['truck']):>15}\")\n",
        "    print(f\"{'Total Buses':<30} {yolov5m_metrics['summary']['total_buses']:>15} {sum(metrics['class_counts']['bus']):>15}\")\n",
        "else:\n",
        "    print(f\"{'Total Frames':<30} {'N/A':>15} {len(metrics['frame_indices']):>15}\")\n",
        "    print(f\"{'Total Detections':<30} {'N/A':>15} {sum(metrics['detections_per_frame']):>15}\")\n",
        "    print(f\"{'Avg Detections/Frame':<30} {'N/A':>15} {np.mean(metrics['detections_per_frame']):>15.2f}\")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nVisualizations saved to: {viz_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6819099",
      "metadata": {},
      "source": [
        "## Export YOLO-SwinV2 Metrics\n",
        "\n",
        "Save the inference metrics to JSON for future reference and comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507de7aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export YOLO-SwinV2 metrics to JSON\n",
        "metrics_export = {\n",
        "    \"model_name\": \"YOLO-SwinV2 (Trained on AAU RainSnow)\",\n",
        "    \"config\": {\n",
        "        \"conf_threshold\": CONF_THRESH,\n",
        "        \"iou_match_threshold\": IOU_MATCH_THRESH,\n",
        "        \"vehicle_classes\": list(VEHICLE_CLASSES.values()),\n",
        "        \"weights_path\": WEIGHTS_PATH,\n",
        "    },\n",
        "    \"summary\": {\n",
        "        \"total_frames\": len(metrics[\"frame_indices\"]),\n",
        "        \"total_detections\": sum(metrics[\"detections_per_frame\"]),\n",
        "        \"avg_detections_per_frame\": float(np.mean(metrics[\"detections_per_frame\"])),\n",
        "        \"mean_confidence\": float(np.mean(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"std_confidence\": float(np.std(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"min_confidence\": float(min(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"max_confidence\": float(max(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"total_cars\": sum(metrics[\"class_counts\"][\"car\"]),\n",
        "        \"total_trucks\": sum(metrics[\"class_counts\"][\"truck\"]),\n",
        "        \"total_buses\": sum(metrics[\"class_counts\"][\"bus\"]),\n",
        "        \"unique_tracks\": next_track_id,\n",
        "    },\n",
        "    \"per_frame_data\": {\n",
        "        \"frame_indices\": metrics[\"frame_indices\"],\n",
        "        \"detections_per_frame\": metrics[\"detections_per_frame\"],\n",
        "        \"class_counts\": metrics[\"class_counts\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "metrics_path = os.path.join(OUTPUT_DIR, \"YOLO_SwinV2_metrics.json\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_export, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Metrics exported to: {metrics_path}\")\n",
        "print(f\"\\nYou can compare these metrics with YOLO-V5m results.\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  - Video with tracking: {output_mp4}\")\n",
        "print(f\"  - Metrics JSON: {metrics_path}\")\n",
        "print(f\"  - Visualizations: {viz_dir}/\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
