{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6c361898",
      "metadata": {
        "id": "6c361898"
      },
      "source": [
        "# Vehicle Detection with YOLO-SwinV2 Model\n",
        "\n",
        "This notebook implements a modified YOLO model with SwinV2-Tiny as the backbone for vehicle detection. The model is trained on the AAU RainSnow dataset (vehicles in rainy/snowy conditions) and evaluated on the same highway video as the pre-trained YOLO-V5m model.\n",
        "\n",
        "**Key Features:**\n",
        "- SwinV2-Tiny backbone replacing the original CSPDarknet backbone\n",
        "- Training on weather-degraded vehicle data for improved detection in adverse conditions\n",
        "- Metrics collection for comparison with the pre-trained YOLO-V5m baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e15d03",
      "metadata": {
        "id": "d0e15d03"
      },
      "source": [
        "## How To Run\n",
        "\n",
        "It is recommended to run this notebook in Google Colab. However, it is implemented so that it can also be run in a local environment.\n",
        "\n",
        "**To run this notebook in Google Colab:**\n",
        "- Download the whole project folder (enhanced_vehicle_detection) from GitHub.\n",
        "- Place it in MyDrive in Google Drive.\n",
        "    - If the project folder is placed in a different path in Google Drive, the paths for the input video and outputs need to be edited accordingly.\n",
        "- All set! You can now run the cells.\n",
        "\n",
        "**To run this notebook in a local environment:**\n",
        "- Fork or clone the GitHub repository.\n",
        "- Run `pip install -r app/requirements.txt` to install all required libraries.\n",
        "- Since the code requires video conversion, make sure to install **ffmpeg**:\n",
        "    - macOS: `brew install ffmpeg`\n",
        "    - Ubuntu/Linux: `sudo apt install ffmpeg`\n",
        "    - Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html)\n",
        "- All set! You can now run the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e60300e",
      "metadata": {
        "id": "5e60300e"
      },
      "source": [
        "## Setup YOLO V5\n",
        "\n",
        "The code below installs every required libraries to load and use YOLO-V5 model. This code only need to be run once while using this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Jp2lgmXRjRlS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp2lgmXRjRlS",
        "outputId": "db5ef678-ca6d-4b50-9a55-f136e348731d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone -q https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "\n",
        "!pip install -q -r requirements.txt opencv-python-headless==4.10.0.84 timm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51co0qE1jbl5",
      "metadata": {
        "id": "51co0qE1jbl5"
      },
      "source": [
        "## Clean YOLO V5 directory\n",
        "\n",
        "If there is any old patches applied to the original YOLO V5 files, remove them and restore back to the original file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "yAR_TTZsjaHn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAR_TTZsjaHn",
        "outputId": "f0c51cf2-2d11-494b-96a6-73ac87b7d3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'yolov5'\n",
            "/content/yolov5\n",
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ],
      "source": [
        "%cd yolov5\n",
        "!git status\n",
        "\n",
        "# Reset modified core files (safe and important)\n",
        "!git checkout -- models/yolo.py models/common.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a5b7125",
      "metadata": {
        "id": "1a5b7125"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e893fec7",
      "metadata": {
        "id": "e893fec7"
      },
      "outputs": [],
      "source": [
        "import cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.ops import nms\n",
        "from collections import defaultdict\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "import timm\n",
        "import sys\n",
        "import json, shutil\n",
        "import os, pandas as pd, glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7b7ced",
      "metadata": {
        "id": "bb7b7ced"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up paths based on whether running in Google Colab or local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab2bf134",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab2bf134",
        "outputId": "a66a0b17-d627-4035-bb66-0805439a2081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Random seed set to: 42\n",
            "Using device: cuda\n",
            "Data root: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/training_data_vehicles_in_rain\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Google Colab\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Paths for Colab\n",
        "    DATA_ROOT = Path('/content/drive/MyDrive/enhanced_vehicle_detection/data/training_data_vehicles_in_rain')\n",
        "    VIDEO_PATH = Path('/content/drive/MyDrive/enhanced_vehicle_detection/data/rainy_highway_video.mp4')\n",
        "    PROJECT_ROOT = Path('/content/drive/MyDrive/enhanced_vehicle_detection')\n",
        "else:\n",
        "    # Paths for local environment\n",
        "    DATA_ROOT = Path('../data/training_data_vehicles_in_rain')\n",
        "    VIDEO_PATH = Path('../data/rainy_highway_video.mp4')\n",
        "    PROJECT_ROOT = Path('../')\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "      # For multi-GPU\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # For deterministic behavior\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Set environment variable for additional reproducibility\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Random seed set to: {SEED}\")\n",
        "# ============================================================================\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4n3l7Hylkf2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n3l7Hylkf2b",
        "outputId": "5c0e88b9-122b-4cd5-d340-c97cacbe8c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting models/swinv2_backbone.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile models/swinv2_backbone.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from models.common import Conv\n",
        "\n",
        "class SwinV2Backbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-output SwinV2 backbone for YOLOv5.\n",
        "    - Takes 3xHxW RGB images (any size).\n",
        "    - Resizes to expected size for SwinV2.\n",
        "    - Uses timm SwinV2 Tiny.\n",
        "    - Grabs the last stage feature map.\n",
        "    - Projects to c2 channels and resizes output for YOLO.\n",
        "    \"\"\"\n",
        "    def __init__(self, c1, c2, model_name=\"swinv2_tiny_window16_256\", out_index=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Expected input size for the SwinV2 model (256 for swinv2_tiny_window16_256)\n",
        "        self.expected_size = 256\n",
        "\n",
        "        # features_only=True for returning a list of feature maps\n",
        "        self.swin = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            features_only=True,\n",
        "            out_indices=(out_index,)\n",
        "        )\n",
        "        in_ch = self.swin.feature_info.channels()[0]\n",
        "\n",
        "        # Project Swin channels for c2 channels used by YOLO head\n",
        "        self.proj = Conv(in_ch, c2, k=1, s=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store original size\n",
        "        _, _, orig_h, orig_w = x.shape\n",
        "        \n",
        "        # SwinV2 requires fixed input size\n",
        "        if orig_h != self.expected_size or orig_w != self.expected_size:\n",
        "            x = F.interpolate(x, size=(self.expected_size, self.expected_size), \n",
        "                            mode='bilinear', align_corners=False)\n",
        "        \n",
        "        # Extract the tensor from the list with 1 tensor\n",
        "        feats = self.swin(x)\n",
        "        f = feats[0]\n",
        "\n",
        "        # Convert NHWC to NCHW as YOLO expects\n",
        "        if f.dim() == 4:\n",
        "            f = f.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Project to output channels\n",
        "        out = self.proj(f)\n",
        "        \n",
        "        # Resize output to match expected stride-32 feature map size\n",
        "        expected_out_h = orig_h // 32\n",
        "        expected_out_w = orig_w // 32\n",
        "        _, _, out_h, out_w = out.shape\n",
        "        \n",
        "        if out_h != expected_out_h or out_w != expected_out_w:\n",
        "            out = F.interpolate(out, size=(expected_out_h, expected_out_w), \n",
        "                              mode='bilinear', align_corners=False)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4834a5e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set yolo.py path based on environment\n",
        "if IN_COLAB:\n",
        "    file_path = \"/content/yolov5/models/yolo.py\"\n",
        "else:\n",
        "    # Assuming we're already in the yolov5 directory after %cd yolov5\n",
        "    file_path = \"models/yolo.py\"\n",
        "\n",
        "insert_line = \"from models.swinv2_backbone import SwinV2Backbone\\n\"\n",
        "\n",
        "# Read file\n",
        "with open(file_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Only insert if the importing SwinV2Backbone is not already present\n",
        "if insert_line not in \"\".join(lines):\n",
        "\n",
        "    new_lines = []\n",
        "    inserted = False\n",
        "\n",
        "    for line in lines:\n",
        "        # Before the common import, insert our Swin import\n",
        "        if line.startswith(\"from models.common import\"):\n",
        "            new_lines.append(insert_line)\n",
        "            inserted = True\n",
        "\n",
        "        new_lines.append(line)\n",
        "\n",
        "    if inserted:\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.writelines(new_lines)\n",
        "        print(\"Successfully inserted SwinV2Backbone import before models.common.\")\n",
        "    else:\n",
        "        print(\"Could not find 'from models.common import (' in yolo.py.\")\n",
        "else:\n",
        "    print(f\"SwinV2Backbone import already present — no changes made.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9rnd_lkskkFz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rnd_lkskkFz",
        "outputId": "2645dfd4-f155-4083-9aba-14039f7ece39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Patched models/yolo.py for SwinV2Backbone\n"
          ]
        }
      ],
      "source": [
        "yolo_path = Path(\"models/yolo.py\")\n",
        "text = yolo_path.read_text()\n",
        "import_line = \"from models.swinv2_backbone import SwinV2Backbone\\n\"\n",
        "\n",
        "# Patch parse_model to handle SwinV2Backbone\n",
        "snippet = \"\"\"        elif m is Contract:\n",
        "            c2 = ch[f] * args[0] ** 2\n",
        "        elif m is Expand:\n",
        "            c2 = ch[f] // args[0] ** 2\n",
        "        else:\n",
        "            c2 = ch[f]\n",
        "\"\"\"\n",
        "\n",
        "replacement = \"\"\"        elif m is Contract:\n",
        "            c2 = ch[f] * args[0] ** 2\n",
        "        elif m is Expand:\n",
        "            c2 = ch[f] // args[0] ** 2\n",
        "        elif m is SwinV2Backbone:\n",
        "            # args: [c2, model_name, out_index, pretrained]\n",
        "            c1, c2 = ch[f], args[0]\n",
        "            args = [c1, c2, *args[1:]]\n",
        "        else:\n",
        "            c2 = ch[f]\n",
        "\"\"\"\n",
        "\n",
        "if snippet not in text:\n",
        "    raise RuntimeError(\"Expected snippet not found in models/yolo.py. YOLOv5 version mismatch?\")\n",
        "text = text.replace(snippet, replacement)\n",
        "\n",
        "yolo_path.write_text(text)\n",
        "print(\"Patched models/yolo.py for SwinV2Backbone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jc_-SWyAkpwB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc_-SWyAkpwB",
        "outputId": "0f7061eb-8630-4153-af94-41a7d9ad20b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created models/yolov5m_swinv2.yaml\n"
          ]
        }
      ],
      "source": [
        "swin_yaml = r\"\"\"\n",
        "# YOLOv5 + SwinV2 Tiny backbone (single-scale detection)\n",
        "nc: 3\n",
        "\n",
        "# Single scale anchors - only one set for single-scale detection\n",
        "anchors:\n",
        "  - [30,61, 62,45, 59,119]\n",
        "\n",
        "depth_multiple: 1.0\n",
        "width_multiple: 1.0\n",
        "\n",
        "backbone:\n",
        "  # args: [c2, model_name, out_index, pretrained]\n",
        "  # c1 is automatically set from previous layer channels\n",
        "  - [-1, 1, SwinV2Backbone, [256, 'swinv2_tiny_window16_256', 3, true]]\n",
        "\n",
        "head:\n",
        "  # C3 args: [c2] - n is automatically handled by the repeat count (2nd value)\n",
        "  - [0, 1, C3, [256]]\n",
        "  # Single-scale Detect: input from layer 1 only, uses single anchor set\n",
        "  - [[1], 1, Detect, [nc, anchors]]\n",
        "\"\"\"\n",
        "\n",
        "Path(\"models/yolov5m_swinv2.yaml\").write_text(swin_yaml)\n",
        "print(\"Created models/yolov5m_swinv2.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHLbvL4WktFD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHLbvL4WktFD",
        "outputId": "f1754f69-ebf9-4a7e-9665-6ad03c287405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ data/vehicles.yaml written\n"
          ]
        }
      ],
      "source": [
        "dataset_yaml = f\"\"\"\n",
        "train: /content/data/AAU_YOLO/images/train\n",
        "val: /content/data/AAU_YOLO/images/val\n",
        "nc: 3\n",
        "names: ['car', 'truck', 'bus']\n",
        "\"\"\"\n",
        "\n",
        "Path(\"data/vehicles.yaml\").write_text(dataset_yaml)\n",
        "print(\"data/vehicles.yaml written\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PTOvQnnmqxwp",
      "metadata": {
        "id": "PTOvQnnmqxwp"
      },
      "source": [
        "## Process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tg7Ljl7epaD3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7Ljl7epaD3",
        "outputId": "a7d74ed1-4562-4272-896c-cb3996e9f4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ Output folders created: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/AAU_YOLO\n"
          ]
        }
      ],
      "source": [
        "json_path = DATA_ROOT/\"aauRainSnow-rgb.json\"\n",
        "\n",
        "output_root = PROJECT_ROOT/\"data/AAU_YOLO\"\n",
        "output_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create folder structure\n",
        "for folder in [\"images/train\",\"images/val\",\"labels/train\",\"labels/val\"]:\n",
        "    (output_root/folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Output folders created:\", output_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ugtU93JbrgT-",
      "metadata": {
        "id": "ugtU93JbrgT-"
      },
      "source": [
        "## Load JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ZeFkkOrdQW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ZeFkkOrdQW",
        "outputId": "1260a314-ded1-4ddf-b2b8-d5ce60b21a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JSON: /content/drive/MyDrive/DL/enhanced_vehicle_detection/data/training_data_vehicles_in_rain/aauRainSnow-rgb.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading JSON:\", json_path)\n",
        "with open(json_path) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "images = {im[\"id\"]: im for im in coco[\"images\"]}\n",
        "\n",
        "# Map AAU classes to YOLO classes\n",
        "VEHICLE_MAP = {3:0, 6:1, 8:2}  # car=0, truck=1, bus=2\n",
        "\n",
        "# Collect annotations per image\n",
        "anns = {}\n",
        "for ann in coco[\"annotations\"]:\n",
        "    if ann[\"category_id\"] in VEHICLE_MAP:\n",
        "        anns.setdefault(ann[\"image_id\"], []).append(ann)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "txh17-bzrmC7",
      "metadata": {
        "id": "txh17-bzrmC7"
      },
      "source": [
        "## Train/Val Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "G-IRKFGnpqmv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-IRKFGnpqmv",
        "outputId": "2e8c92dd-f70f-493d-bcda-bc018f8f6078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 1597, Val images: 400\n"
          ]
        }
      ],
      "source": [
        "image_ids = list(anns.keys())\n",
        "np.random.shuffle(image_ids)\n",
        "\n",
        "split = int(0.8 * len(image_ids))\n",
        "train_ids = image_ids[:split]\n",
        "val_ids = image_ids[split:]\n",
        "\n",
        "print(f\"Train images: {len(train_ids)}, Val images: {len(val_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5qtmL54Jptek",
      "metadata": {
        "id": "5qtmL54Jptek"
      },
      "outputs": [],
      "source": [
        "def coco_to_yolo(img_id, img_dir, lbl_dir):\n",
        "    info = images[img_id]\n",
        "\n",
        "    src_img = DATA_ROOT / info[\"file_name\"]\n",
        "    fname = Path(info[\"file_name\"]).name\n",
        "\n",
        "    # Copy image\n",
        "    dst_img = img_dir / fname\n",
        "    shutil.copy2(src_img, dst_img)\n",
        "\n",
        "    w, h = info[\"width\"], info[\"height\"]\n",
        "\n",
        "    # Create YOLO label\n",
        "    dst_lbl = lbl_dir / (fname.replace(\".png\", \".txt\").replace(\".jpg\",\".txt\"))\n",
        "\n",
        "    lines = []\n",
        "    for ann in anns[img_id]:\n",
        "        cls = VEHICLE_MAP[ann[\"category_id\"]]\n",
        "        x, y, bw, bh = ann[\"bbox\"]\n",
        "\n",
        "        xc = (x + bw/2) / w\n",
        "        yc = (y + bh/2) / h\n",
        "        bw /= w\n",
        "        bh /= h\n",
        "\n",
        "        lines.append(f\"{cls} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\")\n",
        "\n",
        "    dst_lbl.write_text(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2GtsDdpwYv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d2GtsDdpwYv",
        "outputId": "9a6e7322-8a58-4e6e-b9ed-1569cce0b809"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train: 100%|██████████| 1597/1597 [48:49<00:00,  1.83s/it]\n",
            "Processing val: 100%|██████████| 400/400 [12:38<00:00,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ Dataset conversion completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def process(ids, split):\n",
        "    img_dir = output_root / f\"images/{split}\"\n",
        "    lbl_dir = output_root / f\"labels/{split}\"\n",
        "\n",
        "    for img_id in tqdm(ids, desc=f\"Processing {split}\"):\n",
        "        coco_to_yolo(img_id, img_dir, lbl_dir)\n",
        "\n",
        "process(train_ids, \"train\")\n",
        "process(val_ids, \"val\")\n",
        "\n",
        "print(\"Dataset conversion completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UPgiS20Ypzsx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPgiS20Ypzsx",
        "outputId": "8121fe6f-6b3d-40dd-cc96-d18fcfb0353b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ vehicles.yaml created\n"
          ]
        }
      ],
      "source": [
        "dataset_yaml = f\"\"\"\n",
        "train: {output_root}/images/train\n",
        "val: {output_root}/images/val\n",
        "\n",
        "nc: 3\n",
        "names: ['car','truck','bus']\n",
        "\"\"\"\n",
        "\n",
        "# Write vehicles.yaml to yolov5 data directory\n",
        "if IN_COLAB:\n",
        "    vehicles_yaml_path = Path(\"/content/yolov5/data/vehicles.yaml\")\n",
        "else:\n",
        "    vehicles_yaml_path = Path(\"data/vehicles.yaml\")\n",
        "\n",
        "vehicles_yaml_path.write_text(dataset_yaml)\n",
        "print(f\"vehicles.yaml created at: {vehicles_yaml_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JbL6TVoW9YLS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbL6TVoW9YLS",
        "outputId": "e7e094c5-3c75-48bb-a72d-19eee8498729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from models.swinv2_backbone import SwinV2Backbone\n",
            "\n",
            "        elif m is SwinV2Backbone:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set yolo.py path based on environment\n",
        "if IN_COLAB:\n",
        "    file_path = \"/content/yolov5/models/yolo.py\"\n",
        "else:\n",
        "    file_path = \"models/yolo.py\"\n",
        "\n",
        "# Verify SwinV2Backbone import was added\n",
        "with open(file_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(\"Checking for SwinV2Backbone in yolo.py:\")\n",
        "for line in lines:\n",
        "    if \"SwinV2Backbone\" in line:\n",
        "        print(f\"{line.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7a3f21",
      "metadata": {},
      "source": [
        "## Training: YOLO with SwinV2 Backbone\n",
        "\n",
        "This notebook trains a **modified YOLO model with SwinV2-Tiny backbone** replacing the original CSPDarknet backbone.\n",
        "\n",
        "### Key Architecture Change:\n",
        "| Component | Original YOLOv5 | YOLO-SwinV2 (This Project) |\n",
        "|-----------|----------------|---------------------------|\n",
        "| Backbone | CSPDarknet53 | **SwinV2-Tiny** (ImageNet pretrained) |\n",
        "| Neck | FPN + PAN | Single-scale C3 |\n",
        "| Head | Multi-scale Detect | Single-scale Detect |\n",
        "\n",
        "### Training Strategy:\n",
        "The SwinV2 backbone is **pretrained on ImageNet** via the `timm` library, providing excellent feature extraction capabilities. Only the detection head is trained from scratch on the AAU RainSnow dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Option A: Fine-tune Standard YOLOv5m\n",
        "> **Note**: The cells immediately below fine-tune standard YOLOv5m (CSPDarknet backbone), NOT SwinV2. \n",
        "> **Skip cells 26-31 and go directly to Cell 32** for training with SwinV2 backbone. fine-tuning data\n",
        "- Standard YOLOv5 architecture (no SwinV2 backbone)\n",
        "\n",
        "### Option B: Train YOLO-SwinV2 from Scratch\n",
        "- Custom architecture with SwinV2-Tiny backbone\n",
        "- Trained only on AAU RainSnow (~1,600 images)\n",
        "- Requires many more epochs for good results\n",
        "- Experimental architecture comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f028ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION A: FINE-TUNE PRETRAINED YOLOv5m ON AAU RAINSNOW\n",
        "# This uses transfer learning: COCO pretrained weights + fine-tuning on rainy data\n",
        "# The model already knows how to detect vehicles and just adapt it to rainy conditions\n",
        "\n",
        "# Change to yolov5 directory\n",
        "import os\n",
        "yolov5_dir = \"/content/yolov5\" if IN_COLAB else \"yolov5\"\n",
        "os.chdir(yolov5_dir)\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Create a dataset config that maps COCO vehicle classes\n",
        "# COCO classes: car=2, truck=7, bus=5. Train with nc=80 and filter at inference\n",
        "\n",
        "# Create vehicles dataset YAML (keeping original COCO class structure for fine-tuning)\n",
        "vehicles_finetune_yaml = f\"\"\"\n",
        "# AAU RainSnow dataset for fine-tuning (uses COCO class IDs)\n",
        "train: {output_root}/images/train\n",
        "val: {output_root}/images/val\n",
        "\n",
        "# We use 3 classes for our custom training\n",
        "nc: 3\n",
        "names: ['car', 'truck', 'bus']\n",
        "\"\"\"\n",
        "\n",
        "Path(\"data/vehicles_finetune.yaml\").write_text(vehicles_finetune_yaml)\n",
        "print(\"Created data/vehicles_finetune.yaml for fine-tuning\")\n",
        "\n",
        "# Training hyperparameters for fine-tuning\n",
        "FINETUNE_EPOCHS = 20\n",
        "FINETUNE_BATCH = 16\n",
        "FINETUNE_IMG_SIZE = 640   # Standard YOLOv5 size\n",
        "\n",
        "print(f\"\\nFine-tuning Configuration:\")\n",
        "print(f\"  - Base model: YOLOv5m (pretrained on COCO)\")\n",
        "print(f\"  - Fine-tune dataset: AAU RainSnow\")\n",
        "print(f\"  - Epochs: {FINETUNE_EPOCHS}\")\n",
        "print(f\"  - Image size: {FINETUNE_IMG_SIZE}\")\n",
        "print(f\"  - Batch size: {FINETUNE_BATCH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226c5160",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN FINE-TUNING (Option A)\n",
        "# Key difference: --weights yolov5m.pt (pretrained) instead of '' (from scratch)\n",
        "\n",
        "!python train.py \\\n",
        "  --img {FINETUNE_IMG_SIZE} \\\n",
        "  --batch {FINETUNE_BATCH} \\\n",
        "  --epochs {FINETUNE_EPOCHS} \\\n",
        "  --data data/vehicles_finetune.yaml \\\n",
        "  --weights yolov5m.pt \\\n",
        "  --project YOLO_Finetuned \\\n",
        "  --name rainsnow_finetuned \\\n",
        "  --cache \\\n",
        "  --exist-ok\n",
        "\n",
        "print(\"\\nFine-tuning complete!\")\n",
        "print(\"Weights saved to: YOLO_Finetuned/rainsnow_finetuned/weights/best.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d833a9",
      "metadata": {},
      "source": [
        "## Training Metrics Visualization\n",
        "\n",
        "YOLOv5 automatically logs the following metrics during training:\n",
        "- **Training Losses**: Box loss, Objectness loss, Classification loss\n",
        "- **Validation Losses**: Same three losses on validation set\n",
        "- **Evaluation Metrics**: Precision, Recall, mAP@0.5, mAP@0.5:0.95\n",
        "\n",
        "These are saved to `results.csv` in the training output folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e58e1ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD AND VISUALIZE TRAINING METRICS\n",
        "\n",
        "# Find the training results\n",
        "results_files = sorted(glob.glob(\"YOLO_Finetuned/rainsnow_finetuned*/results.csv\"))\n",
        "\n",
        "if not results_files:\n",
        "    results_files = sorted(glob.glob(\"YOLO_SwinV2/rainsnow_swinv2*/results.csv\"))\n",
        "\n",
        "if results_files:\n",
        "    results_path = results_files[-1]\n",
        "    print(f\"Loading training results from: {results_path}\")\n",
        "    \n",
        "    # Load results\n",
        "    df = pd.read_csv(results_path)\n",
        "    # Remove whitespace from column names\n",
        "    df.columns = df.columns.str.strip()\n",
        "    \n",
        "    print(f\"\\nAvailable metrics: {list(df.columns)}\")\n",
        "    print(f\"Total epochs: {len(df)}\")\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('YOLOv5 Training Metrics', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    epochs = df.index + 1\n",
        "    \n",
        "    # 1. Training Losses\n",
        "    ax1 = axes[0, 0]\n",
        "    if 'train/box_loss' in df.columns:\n",
        "        ax1.plot(epochs, df['train/box_loss'], label='Box Loss', color='#E63946', linewidth=2)\n",
        "        ax1.plot(epochs, df['train/obj_loss'], label='Objectness Loss', color='#2E86AB', linewidth=2)\n",
        "        ax1.plot(epochs, df['train/cls_loss'], label='Class Loss', color='#F4A261', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training Losses')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Validation Losses\n",
        "    ax2 = axes[0, 1]\n",
        "    if 'val/box_loss' in df.columns:\n",
        "        ax2.plot(epochs, df['val/box_loss'], label='Box Loss', color='#E63946', linewidth=2)\n",
        "        ax2.plot(epochs, df['val/obj_loss'], label='Objectness Loss', color='#2E86AB', linewidth=2)\n",
        "        ax2.plot(epochs, df['val/cls_loss'], label='Class Loss', color='#F4A261', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_title('Validation Losses')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Combined Train vs Val Loss\n",
        "    ax3 = axes[0, 2]\n",
        "    if 'train/box_loss' in df.columns and 'val/box_loss' in df.columns:\n",
        "        total_train = df['train/box_loss'] + df['train/obj_loss'] + df['train/cls_loss']\n",
        "        total_val = df['val/box_loss'] + df['val/obj_loss'] + df['val/cls_loss']\n",
        "        ax3.plot(epochs, total_train, label='Training Loss', color='#2E86AB', linewidth=2)\n",
        "        ax3.plot(epochs, total_val, label='Validation Loss', color='#E63946', linewidth=2)\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Total Loss')\n",
        "    ax3.set_title('Training vs Validation Loss')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Precision & Recall\n",
        "    ax4 = axes[1, 0]\n",
        "    if 'metrics/precision' in df.columns:\n",
        "        ax4.plot(epochs, df['metrics/precision'], label='Precision', color='#2A9D8F', linewidth=2)\n",
        "        ax4.plot(epochs, df['metrics/recall'], label='Recall', color='#E76F51', linewidth=2)\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Score')\n",
        "    ax4.set_title('Precision & Recall')\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. mAP Scores\n",
        "    ax5 = axes[1, 1]\n",
        "    if 'metrics/mAP_0.5' in df.columns:\n",
        "        ax5.plot(epochs, df['metrics/mAP_0.5'], label='mAP@0.5', color='#264653', linewidth=2)\n",
        "        ax5.plot(epochs, df['metrics/mAP_0.5:0.95'], label='mAP@0.5:0.95', color='#E9C46A', linewidth=2)\n",
        "    ax5.set_xlabel('Epoch')\n",
        "    ax5.set_ylabel('mAP')\n",
        "    ax5.set_title('Mean Average Precision (mAP)')\n",
        "    ax5.set_ylim(0, 1)\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Learning Rate\n",
        "    ax6 = axes[1, 2]\n",
        "    lr_cols = [c for c in df.columns if 'lr' in c.lower()]\n",
        "    if lr_cols:\n",
        "        for col in lr_cols:\n",
        "            ax6.plot(epochs, df[col], label=col.split('/')[-1], linewidth=2)\n",
        "    ax6.set_xlabel('Epoch')\n",
        "    ax6.set_ylabel('Learning Rate')\n",
        "    ax6.set_title('Learning Rate Schedule')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save the plot\n",
        "    train_viz_dir = os.path.dirname(results_path)\n",
        "    plt.savefig(os.path.join(train_viz_dir, 'training_metrics_summary.png'), dpi=150, bbox_inches='tight')\n",
        "    \n",
        "    # Also save to output directory\n",
        "    if 'OUTPUT_DIR' in dir():\n",
        "        os.makedirs(os.path.join(OUTPUT_DIR, 'visualizations'), exist_ok=True)\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'visualizations', 'training_metrics.png'), dpi=150, bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\n\")\n",
        "    print(\"           TRAINING SUMMARY\")\n",
        "    print(f\"\\n\")\n",
        "    if 'metrics/mAP_0.5' in df.columns:\n",
        "        best_epoch = df['metrics/mAP_0.5'].idxmax() + 1\n",
        "        print(f\"Best mAP@0.5: {df['metrics/mAP_0.5'].max():.4f} (Epoch {best_epoch})\")\n",
        "        print(f\"Best mAP@0.5:0.95: {df['metrics/mAP_0.5:0.95'].max():.4f}\")\n",
        "    if 'metrics/precision' in df.columns:\n",
        "        print(f\"Best Precision: {df['metrics/precision'].max():.4f}\")\n",
        "        print(f\"Best Recall: {df['metrics/recall'].max():.4f}\")\n",
        "    if 'val/box_loss' in df.columns:\n",
        "        print(f\"\\nFinal Validation Losses:\")\n",
        "        print(f\"  - Box Loss: {df['val/box_loss'].iloc[-1]:.4f}\")\n",
        "        print(f\"  - Obj Loss: {df['val/obj_loss'].iloc[-1]:.4f}\")\n",
        "        print(f\"  - Cls Loss: {df['val/cls_loss'].iloc[-1]:.4f}\")\n",
        "    print(f\"\\n\")\n",
        "    \n",
        "else:\n",
        "    print(\"No training results found. Run training first!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b2adae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXPORT TRAINING METRICS TO JSON\n",
        "\n",
        "if results_files:\n",
        "    # Create training metrics export\n",
        "    training_metrics_export = {\n",
        "        \"model_name\": \"YOLOv5m Fine-tuned on AAU RainSnow\" if 'Finetuned' in results_path else \"YOLO-SwinV2\",\n",
        "        \"training_config\": {\n",
        "            \"epochs\": len(df),\n",
        "            \"base_weights\": \"yolov5m.pt (COCO pretrained)\" if 'Finetuned' in results_path else \"scratch\",\n",
        "            \"dataset\": \"AAU RainSnow\",\n",
        "        },\n",
        "        \"best_metrics\": {\n",
        "            \"mAP_0.5\": float(df['metrics/mAP_0.5'].max()) if 'metrics/mAP_0.5' in df.columns else None,\n",
        "            \"mAP_0.5_0.95\": float(df['metrics/mAP_0.5:0.95'].max()) if 'metrics/mAP_0.5:0.95' in df.columns else None,\n",
        "            \"precision\": float(df['metrics/precision'].max()) if 'metrics/precision' in df.columns else None,\n",
        "            \"recall\": float(df['metrics/recall'].max()) if 'metrics/recall' in df.columns else None,\n",
        "            \"best_epoch\": int(df['metrics/mAP_0.5'].idxmax() + 1) if 'metrics/mAP_0.5' in df.columns else None,\n",
        "        },\n",
        "        \"final_losses\": {\n",
        "            \"train_box_loss\": float(df['train/box_loss'].iloc[-1]) if 'train/box_loss' in df.columns else None,\n",
        "            \"train_obj_loss\": float(df['train/obj_loss'].iloc[-1]) if 'train/obj_loss' in df.columns else None,\n",
        "            \"train_cls_loss\": float(df['train/cls_loss'].iloc[-1]) if 'train/cls_loss' in df.columns else None,\n",
        "            \"val_box_loss\": float(df['val/box_loss'].iloc[-1]) if 'val/box_loss' in df.columns else None,\n",
        "            \"val_obj_loss\": float(df['val/obj_loss'].iloc[-1]) if 'val/obj_loss' in df.columns else None,\n",
        "            \"val_cls_loss\": float(df['val/cls_loss'].iloc[-1]) if 'val/cls_loss' in df.columns else None,\n",
        "        },\n",
        "        \"per_epoch_data\": {\n",
        "            \"epochs\": list(range(1, len(df) + 1)),\n",
        "            \"train_loss\": (df['train/box_loss'] + df['train/obj_loss'] + df['train/cls_loss']).tolist() if 'train/box_loss' in df.columns else [],\n",
        "            \"val_loss\": (df['val/box_loss'] + df['val/obj_loss'] + df['val/cls_loss']).tolist() if 'val/box_loss' in df.columns else [],\n",
        "            \"mAP_0.5\": df['metrics/mAP_0.5'].tolist() if 'metrics/mAP_0.5' in df.columns else [],\n",
        "            \"precision\": df['metrics/precision'].tolist() if 'metrics/precision' in df.columns else [],\n",
        "            \"recall\": df['metrics/recall'].tolist() if 'metrics/recall' in df.columns else [],\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Save to training directory\n",
        "    metrics_json_path = os.path.join(os.path.dirname(results_path), 'training_metrics.json')\n",
        "    with open(metrics_json_path, 'w') as f:\n",
        "        json.dump(training_metrics_export, f, indent=2)\n",
        "    print(f\"Training metrics exported to: {metrics_json_path}\")\n",
        "    \n",
        "    # Also save to output directory\n",
        "    if 'OUTPUT_DIR' in dir():\n",
        "        output_metrics_path = os.path.join(OUTPUT_DIR, 'training_metrics.json')\n",
        "        with open(output_metrics_path, 'w') as f:\n",
        "            json.dump(training_metrics_export, f, indent=2)\n",
        "        print(f\"Also saved to: {output_metrics_path}\")\n",
        "else:\n",
        "    print(\"No training results to export.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156cac97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SET WEIGHTS PATH FOR INFERENCE\n",
        "# This cell is for Option A (standard YOLOv5m fine-tuning)\n",
        "# Skip this cell if you're using YOLO-SwinV2 (go to Cell 35 instead)\n",
        "\n",
        "# Find the fine-tuned weights (Option A - standard YOLOv5m)\n",
        "finetuned_weights = sorted(glob.glob(\"YOLO_Finetuned/rainsnow_finetuned*/weights/best.pt\"))\n",
        "\n",
        "if finetuned_weights:\n",
        "    WEIGHTS_PATH = finetuned_weights[-1]\n",
        "    print(f\"Using fine-tuned YOLOv5m weights: {WEIGHTS_PATH}\")\n",
        "    print(\"(Note: This uses standard CSPDarknet backbone, not SwinV2)\")\n",
        "else:\n",
        "    # Fallback to SwinV2 weights\n",
        "    swinv2_weights = sorted(glob.glob(\"YOLO_SwinV2/rainsnow_swinv2*/weights/best.pt\"))\n",
        "    if swinv2_weights:\n",
        "        WEIGHTS_PATH = swinv2_weights[-1]\n",
        "        print(f\"Fine-tuned weights not found. Using SwinV2 weights: {WEIGHTS_PATH}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No trained weights found! Run training first.\")\n",
        "\n",
        "# Set output directory\n",
        "if IN_COLAB:\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/DL/enhanced_vehicle_detection/outputs/YOLO_Finetuned'\n",
        "else:\n",
        "    OUTPUT_DIR = '../outputs/YOLO_Finetuned'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'visualizations'), exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fdbdb3",
      "metadata": {},
      "source": [
        "---\n",
        "## Option B - Train YOLO-SwinV2\n",
        "---\n",
        "\n",
        "## Train YOLO-SwinV2\n",
        "\n",
        "**This is the main training section for the modified YOLO with SwinV2 backbone.**\n",
        "\n",
        "### Architecture Details:\n",
        "- **Backbone**: SwinV2-Tiny (`swinv2_tiny_window16_256`) - pretrained on ImageNet\n",
        "- **Detection Head**: Custom single-scale head trained on AAU RainSnow\n",
        "- **Input Size**: 256x256 (required by SwinV2 window size)\n",
        "\n",
        "### Why SwinV2 Backbone?\n",
        "- **Shifted Window Attention**: More efficient than global attention\n",
        "- **Hierarchical Features**: Multi-scale representations like CNNs\n",
        "- **Transfer Learning**: ImageNet pretraining provides strong visual features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qswSRTx4kwMh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qswSRTx4kwMh",
        "outputId": "537bf760-4217-4af4-d6cb-295a76d7d4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2025-12-13 04:48:00.862698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765601280.884170   36342 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765601280.890674   36342 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765601280.907167   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765601280.907194   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765601280.907197   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765601280.907200   36342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=models/yolov5m_swinv2.yaml, data=data/vehicles.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=15, batch_size=16, imgsz=256, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=YOLO_SwinV2, name=rainsnow_swinv2, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-450-g781b9d57 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir YOLO_SwinV2', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/train.py\", line 987, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/train.py\", line 688, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov5/train.py\", line 225, in train\n",
            "    model = Model(cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov5/models/yolo.py\", line 242, in __init__\n",
            "    self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov5/models/yolo.py\", line 456, in parse_model\n",
            "    m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n",
            "                                                                    ^^^^^^^^\n",
            "  File \"/content/yolov5/models/swinv2_backbone.py\", line 18, in __init__\n",
            "    self.swin = timm.create_model(\n",
            "                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py\", line 108, in create_model\n",
            "    model_source, model_id = parse_model_name(model_name)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py\", line 20, in parse_model_name\n",
            "    if model_name.startswith('hf_hub'):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'int' object has no attribute 'startswith'\n"
          ]
        }
      ],
      "source": [
        "# TRAIN YOLO-SwinV2 MODEL\n",
        "\n",
        "# Change to yolov5 directory\n",
        "import os\n",
        "yolov5_dir = \"/content/yolov5\" if IN_COLAB else \"yolov5\"\n",
        "os.chdir(yolov5_dir)\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING HYPERPARAMETERS - Adjust these for your time/quality tradeoff\n",
        "SWINV2_EPOCHS = 20\n",
        "SWINV2_BATCH = 16\n",
        "SWINV2_IMG_SIZE = 256    # Required for SwinV2 window16_256\n",
        "\n",
        "print(f\"\\nYOLO-SwinV2 Training Configuration:\")\n",
        "print(f\"  - Backbone: SwinV2-Tiny (ImageNet pretrained)\")\n",
        "print(f\"  - Epochs: {SWINV2_EPOCHS}\")\n",
        "print(f\"  - Batch size: {SWINV2_BATCH}\")\n",
        "print(f\"  - Image size: {SWINV2_IMG_SIZE}x{SWINV2_IMG_SIZE}\")\n",
        "print(f\"  - Dataset: AAU RainSnow (vehicles in adverse weather)\")\n",
        "\n",
        "# Train the model\n",
        "!python train.py \\\n",
        "  --img {SWINV2_IMG_SIZE} \\\n",
        "  --batch {SWINV2_BATCH} \\\n",
        "  --epochs {SWINV2_EPOCHS} \\\n",
        "  --data data/vehicles.yaml \\\n",
        "  --cfg models/yolov5m_swinv2.yaml \\\n",
        "  --weights '' \\\n",
        "  --project YOLO_SwinV2 \\\n",
        "  --name rainsnow_swinv2 \\\n",
        "  --cache \\\n",
        "  --exist-ok\n",
        "\n",
        "print(\"\\nYOLO-SwinV2 training complete!\")\n",
        "print(\"Weights saved to: YOLO_SwinV2/rainsnow_swinv2/weights/best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a1cde1",
      "metadata": {},
      "source": [
        "## Video Inference with Trained Model\n",
        "\n",
        "Run the trained YOLO-SwinV2 model on the highway video to detect vehicles. The trained weights from the best epoch will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a19a025",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VIDEO INFERENCE WITH YOLO-SwinV2 MODEL\n",
        "# This uses the trained YOLO model with SwinV2-Tiny backbone\n",
        "\n",
        "# Find the latest YOLO-SwinV2 training run\n",
        "swinv2_weights = sorted(glob.glob(\"YOLO_SwinV2/rainsnow_swinv2*/weights/best.pt\"))\n",
        "\n",
        "if swinv2_weights:\n",
        "    WEIGHTS_PATH = swinv2_weights[-1]\n",
        "    print(f\"Using YOLO-SwinV2 weights: {WEIGHTS_PATH}\")\n",
        "    print(\"(SwinV2-Tiny backbone, trained on AAU RainSnow)\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No YOLO-SwinV2 weights found! Run Cell 33 (training) first.\")\n",
        "\n",
        "# Set output directory\n",
        "if IN_COLAB:\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/DL/enhanced_vehicle_detection/outputs/YOLO_SwinV2'\n",
        "else:\n",
        "    OUTPUT_DIR = '../outputs/YOLO_SwinV2'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'visualizations'), exist_ok=True)\n",
        "\n",
        "# Run YOLOv5 detect.py on the video\n",
        "output_detect_dir = os.path.join(OUTPUT_DIR, 'detect_output')\n",
        "\n",
        "!python detect.py \\\n",
        "    --weights {WEIGHTS_PATH} \\\n",
        "    --source {VIDEO_PATH} \\\n",
        "    --img 256 \\\n",
        "    --conf-thres 0.25 \\\n",
        "    --iou-thres 0.45 \\\n",
        "    --project {OUTPUT_DIR} \\\n",
        "    --name detect_output \\\n",
        "    --exist-ok \\\n",
        "    --save-txt \\\n",
        "    --save-conf\n",
        "\n",
        "print(f\"\\nDetection complete!\")\n",
        "print(f\"Results saved to: {output_detect_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "546b6204",
      "metadata": {},
      "source": [
        "## Display Detection Video\n",
        "\n",
        "Convert the output video to MP4 format and display it in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2396fc9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking what files were created\n",
        "detect_output_dir = os.path.join(OUTPUT_DIR, 'detect_output')\n",
        "print(f\"Checking directory: {detect_output_dir}\")\n",
        "\n",
        "if os.path.exists(detect_output_dir):\n",
        "    all_files = os.listdir(detect_output_dir)\n",
        "    print(f\"\\nFiles in detect_output:\")\n",
        "    for f in all_files:\n",
        "        filepath = os.path.join(detect_output_dir, f)\n",
        "        size = os.path.getsize(filepath) if os.path.isfile(filepath) else 0\n",
        "        print(f\"  - {f} ({size / 1024:.1f} KB)\")\n",
        "else:\n",
        "    print(\"detect_output directory not found!\")\n",
        "\n",
        "# Also checking the main output dir\n",
        "print(f\"\\nFiles in OUTPUT_DIR ({OUTPUT_DIR}):\")\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    for f in os.listdir(OUTPUT_DIR):\n",
        "        filepath = os.path.join(OUTPUT_DIR, f)\n",
        "        if os.path.isfile(filepath):\n",
        "            size = os.path.getsize(filepath)\n",
        "            print(f\"  - {f} ({size / 1024:.1f} KB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbddba9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detection video - using the file from detect_output\n",
        "detect_video = os.path.join(OUTPUT_DIR, 'detect_output', 'rainy_highway_video.mp4')\n",
        "\n",
        "if os.path.exists(detect_video):\n",
        "    file_size_mb = os.path.getsize(detect_video) / 1024 / 1024\n",
        "    print(f\"Detection video found: {detect_video}\")\n",
        "    print(f\"Size: {file_size_mb:.1f} MB\")\n",
        "    \n",
        "    if file_size_mb > 25:\n",
        "        compressed_video = os.path.join(OUTPUT_DIR, 'detection_compressed.mp4')\n",
        "        print(\"\\nCreating compressed version for display...\")\n",
        "        os.system(f'ffmpeg -y -i \"{detect_video}\" -vcodec libx264 -crf 28 -preset fast -vf scale=640:-2 \"{compressed_video}\" -loglevel error')\n",
        "        \n",
        "        if os.path.exists(compressed_video) and os.path.getsize(compressed_video) > 1000:\n",
        "            print(f\"Compressed: {os.path.getsize(compressed_video)/1024/1024:.1f} MB\")\n",
        "            video_to_display = compressed_video\n",
        "        else:\n",
        "            video_to_display = detect_video\n",
        "    else:\n",
        "        video_to_display = detect_video\n",
        "    \n",
        "    print(\"\\nYOLO-SwinV2 Vehicle Detection Result:\\n\")\n",
        "    video_data = open(video_to_display, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "    display(HTML(f'''\n",
        "        <video width=\"800\" controls autoplay muted>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "            Your browser does not support the video tag.\n",
        "        </video>\n",
        "    '''))\n",
        "else:\n",
        "    print(f\"Video not found at: {detect_video}\")\n",
        "    # List what's available\n",
        "    print(\"\\nAvailable files in OUTPUT_DIR:\")\n",
        "    for f in os.listdir(OUTPUT_DIR):\n",
        "        print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcc619b",
      "metadata": {},
      "source": [
        "## Enhanced Detection with Tracking and Optical Flow\n",
        "\n",
        "Apply the same post-processing pipeline as YOLO-V5m:\n",
        "- **IoU-based tracking**: Match detections across frames using Intersection over Union\n",
        "- **Farneback Optical Flow**: Compute dense motion between frames for smoother tracking\n",
        "- **Track management**: Create, update, and remove tracks with unique IDs\n",
        "- **Visualization**: Draw bounding boxes with track IDs and motion trails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a122828",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ENHANCED DETECTION WITH TRACKING (Same pipeline as YOLO-V5m)\n",
        "\n",
        "yolov5_path = '/content/yolov5' if IN_COLAB else 'yolov5'\n",
        "sys.path.insert(0, yolov5_path)\n",
        "\n",
        "\n",
        "# DETECTION QUALITY SETTINGS\n",
        "FPS = 25\n",
        "\n",
        "# Detect the model type used\n",
        "is_swinv2 = 'YOLO_SwinV2' in WEIGHTS_PATH if 'WEIGHTS_PATH' in dir() else True\n",
        "\n",
        "if is_swinv2:\n",
        "    # Settings for YOLO-SwinV2 model\n",
        "    CONF_THRESH = 0.20        # Lower threshold (detection head trained from scratch)\n",
        "    NMS_IOU_THRESH = 0.30     # More aggressive NMS to reduce overlaps\n",
        "    MIN_BOX_AREA = 300        \n",
        "    print(\"Using settings for YOLO-SwinV2 model (SwinV2-Tiny backbone)\")\n",
        "else:\n",
        "    # Settings for standard fine-tuned YOLOv5m\n",
        "    CONF_THRESH = 0.40        \n",
        "    NMS_IOU_THRESH = 0.45     \n",
        "    MIN_BOX_AREA = 400        \n",
        "    print(\"Using settings for fine-tuned YOLOv5m (CSPDarknet backbone)\")\n",
        "\n",
        "IOU_MATCH_THRESH = 0.3    # For tracking between frames\n",
        "MAX_DETECTIONS = 30       # Maximum detections per frame\n",
        "\n",
        "VEHICLE_CLASSES = {0: 'car', 1: 'truck', 2: 'bus'}\n",
        "\n",
        "print(f\"\\nDetection Quality Settings:\")\n",
        "print(f\"  - Confidence threshold: {CONF_THRESH}\")\n",
        "print(f\"  - NMS IoU threshold: {NMS_IOU_THRESH}\")\n",
        "print(f\"  - Min box area: {MIN_BOX_AREA} pixels\")\n",
        "print(f\"  - Max detections/frame: {MAX_DETECTIONS}\")\n",
        "\n",
        "# Load the trained model\n",
        "print(\"\\nLoading trained YOLO-SwinV2 model...\")\n",
        "model = torch.hub.load(yolov5_path, 'custom', path=WEIGHTS_PATH, source='local')\n",
        "model.conf = CONF_THRESH\n",
        "model.iou = NMS_IOU_THRESH\n",
        "print(f\"Model loaded from: {WEIGHTS_PATH}\")\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "def detect_vehicles(frame):\n",
        "    \"\"\"Run inference on a frame and return filtered detections.\"\"\"\n",
        "    results = model(frame[:, :, ::-1])  # BGR to RGB\n",
        "    \n",
        "    raw_detections = results.xyxy[0].cpu()\n",
        "    \n",
        "    if len(raw_detections) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Extract boxes, scores, and classes\n",
        "    boxes = raw_detections[:, :4]\n",
        "    scores = raw_detections[:, 4]\n",
        "    classes = raw_detections[:, 5].int()\n",
        "    \n",
        "    # Filter by vehicle classes\n",
        "    vehicle_mask = torch.zeros(len(classes), dtype=torch.bool)\n",
        "    for cls_id in VEHICLE_CLASSES.keys():\n",
        "        vehicle_mask |= (classes == cls_id)\n",
        "    \n",
        "    if not vehicle_mask.any():\n",
        "        return []\n",
        "    \n",
        "    boxes = boxes[vehicle_mask]\n",
        "    scores = scores[vehicle_mask]\n",
        "    classes = classes[vehicle_mask]\n",
        "    \n",
        "    # Filter by minimum box area\n",
        "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "    area_mask = areas >= MIN_BOX_AREA\n",
        "    boxes = boxes[area_mask]\n",
        "    scores = scores[area_mask]\n",
        "    classes = classes[area_mask]\n",
        "    \n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Apply additional NMS to remove remaining overlaps\n",
        "    keep_indices = nms(boxes, scores, NMS_IOU_THRESH)\n",
        "    \n",
        "    # Limit max detections (keep highest confidence)\n",
        "    if len(keep_indices) > MAX_DETECTIONS:\n",
        "        # Sort by score and keep top N\n",
        "        sorted_idx = torch.argsort(scores[keep_indices], descending=True)\n",
        "        keep_indices = keep_indices[sorted_idx[:MAX_DETECTIONS]]\n",
        "    \n",
        "    detections = []\n",
        "    for idx in keep_indices:\n",
        "        x1, y1, x2, y2 = boxes[idx].numpy().astype(int)\n",
        "        detections.append({\n",
        "            \"bbox\": [x1, y1, x2, y2],\n",
        "            \"conf\": float(scores[idx]),\n",
        "            \"class\": VEHICLE_CLASSES[int(classes[idx])]\n",
        "        })\n",
        "    \n",
        "    return detections\n",
        "\n",
        "def iou_xyxy(boxA, boxB):\n",
        "    \"\"\"Compute IoU between two boxes in [x1, y1, x2, y2] format.\"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    \n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    \n",
        "    if interArea == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    \n",
        "    return interArea / float(areaA + areaB - interArea)\n",
        "\n",
        "def update_tracks(detections, flow, frame_index):\n",
        "    \"\"\"Update tracks based on detections and optical flow.\"\"\"\n",
        "    global tracks, next_track_id\n",
        "    \n",
        "    unmatched_tracks = set(tracks.keys())\n",
        "    det_to_track = {}\n",
        "    \n",
        "    # IoU-based matching\n",
        "    for det_idx, det in enumerate(detections):\n",
        "        box_det = det[\"bbox\"]\n",
        "        best_iou = 0\n",
        "        best_track = None\n",
        "        \n",
        "        for tid in unmatched_tracks:\n",
        "            box_tr = tracks[tid][\"bbox\"]\n",
        "            iou_val = iou_xyxy(box_det, box_tr)\n",
        "            if iou_val > best_iou:\n",
        "                best_iou = iou_val\n",
        "                best_track = tid\n",
        "        \n",
        "        if best_iou > IOU_MATCH_THRESH:\n",
        "            det_to_track[det_idx] = best_track\n",
        "            unmatched_tracks.remove(best_track)\n",
        "    \n",
        "    # Update matched tracks\n",
        "    for det_idx, track_id in det_to_track.items():\n",
        "        x1, y1, x2, y2 = detections[det_idx][\"bbox\"]\n",
        "        cx = (x1 + x2) // 2\n",
        "        cy = (y1 + y2) // 2\n",
        "        \n",
        "        tracks[track_id][\"bbox\"] = (x1, y1, x2, y2)\n",
        "        tracks[track_id][\"conf\"] = detections[det_idx][\"conf\"]\n",
        "        tracks[track_id][\"class\"] = detections[det_idx][\"class\"]\n",
        "        tracks[track_id][\"trace\"].append((cx, cy))\n",
        "        tracks[track_id][\"last_seen\"] = frame_index\n",
        "    \n",
        "    # Create new tracks for unmatched detections\n",
        "    for det_idx, det in enumerate(detections):\n",
        "        if det_idx in det_to_track:\n",
        "            continue\n",
        "        \n",
        "        x1, y1, x2, y2 = det[\"bbox\"]\n",
        "        cx = (x1 + x2) // 2\n",
        "        cy = (y1 + y2) // 2\n",
        "        \n",
        "        tracks[next_track_id] = {\n",
        "            \"bbox\": (x1, y1, x2, y2),\n",
        "            \"conf\": det[\"conf\"],\n",
        "            \"class\": det[\"class\"],\n",
        "            \"trace\": [(cx, cy)],\n",
        "            \"last_seen\": frame_index\n",
        "        }\n",
        "        next_track_id += 1\n",
        "    \n",
        "    # Remove tracks that haven't been seen recently\n",
        "    max_missing_frames = FPS\n",
        "    tracks_to_remove = [tid for tid, tr in tracks.items() \n",
        "                       if frame_index - tr[\"last_seen\"] > max_missing_frames]\n",
        "    for tid in tracks_to_remove:\n",
        "        del tracks[tid]\n",
        "\n",
        "# Process Video with Tracking\n",
        "\n",
        "print(f\"\\nProcessing video with tracking: {VIDEO_PATH}\")\n",
        "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
        "\n",
        "# Video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS)) or FPS\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f\"Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
        "\n",
        "# Output paths\n",
        "output_avi = os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_with_tracking.avi')\n",
        "output_mp4 = os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_with_tracking.mp4')\n",
        "\n",
        "# Video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(output_avi, fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize tracking\n",
        "tracks = {}\n",
        "next_track_id = 0\n",
        "\n",
        "# Metrics collection\n",
        "metrics = {\n",
        "    \"frame_indices\": [],\n",
        "    \"detections_per_frame\": [],\n",
        "    \"confidence_scores\": [],\n",
        "    \"class_counts\": {\"car\": [], \"truck\": [], \"bus\": []},\n",
        "}\n",
        "\n",
        "# Read first frame for optical flow\n",
        "ret, frame = cap.read()\n",
        "if not ret:\n",
        "    raise RuntimeError(\"Couldn't read the video\")\n",
        "\n",
        "old_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "frame_index = 0\n",
        "pbar = tqdm(total=total_frames, desc='Processing with tracking')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    frame_index += 1\n",
        "    \n",
        "    # Detect vehicles\n",
        "    dets = detect_vehicles(frame)\n",
        "    \n",
        "    # Collect metrics\n",
        "    metrics[\"frame_indices\"].append(frame_index)\n",
        "    metrics[\"detections_per_frame\"].append(len(dets))\n",
        "    \n",
        "    class_count = {\"car\": 0, \"truck\": 0, \"bus\": 0}\n",
        "    for det in dets:\n",
        "        metrics[\"confidence_scores\"].append(det[\"conf\"])\n",
        "        class_count[det[\"class\"]] += 1\n",
        "    \n",
        "    for cls in class_count:\n",
        "        metrics[\"class_counts\"][cls].append(class_count[cls])\n",
        "    \n",
        "    # Compute optical flow\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(\n",
        "        old_gray, frame_gray, None,\n",
        "        0.5,   # pyr_scale\n",
        "        3,     # levels\n",
        "        15,    # winsize\n",
        "        3,     # iterations\n",
        "        5,     # poly_n\n",
        "        1.2,   # poly_sigma\n",
        "        0      # flags\n",
        "    )\n",
        "    \n",
        "    # Update tracks\n",
        "    update_tracks(dets, flow, frame_index)\n",
        "    \n",
        "    # Class-specific colors (BGR format)\n",
        "    CLASS_COLORS = {\n",
        "        'car': (0, 255, 0),      # Green\n",
        "        'truck': (255, 165, 0),   # Orange  \n",
        "        'bus': (255, 0, 255),     # Magenta\n",
        "    }\n",
        "    \n",
        "    # Draw tracks\n",
        "    for track_id, track in tracks.items():\n",
        "        x1, y1, x2, y2 = track[\"bbox\"]\n",
        "        conf = track[\"conf\"]\n",
        "        cls_name = track[\"class\"]\n",
        "        \n",
        "        color = CLASS_COLORS.get(cls_name, (0, 255, 0))\n",
        "        \n",
        "        # Draw bounding box with thicker line\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
        "        \n",
        "        # Draw label background for better visibility\n",
        "        label = f\"ID:{track_id} {cls_name} {conf:.2f}\"\n",
        "        (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "        cv2.rectangle(frame, (x1, y1 - text_h - 10), (x1 + text_w + 4, y1), color, -1)\n",
        "        cv2.putText(frame, label, (x1 + 2, y1 - 5), \n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        \n",
        "        # Draw tracking trail (last 30 points only)\n",
        "        trace = track[\"trace\"][-30:]  # Limit trail length\n",
        "        if len(trace) > 1:\n",
        "            pts = np.array(trace, dtype=np.int32).reshape(-1, 1, 2)\n",
        "            cv2.polylines(frame, [pts], False, (0, 255, 255), 2)\n",
        "    \n",
        "    out.write(frame)\n",
        "    old_gray = frame_gray.copy()\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Convert to MP4\n",
        "print(\"\\nConverting to MP4...\")\n",
        "os.system(f'ffmpeg -y -i \"{output_avi}\" -vcodec libx264 -crf 23 -pix_fmt yuv420p \"{output_mp4}\" -loglevel error')\n",
        "\n",
        "if os.path.exists(output_mp4) and os.path.getsize(output_mp4) > 1000:\n",
        "    print(f\"Video saved to: {output_mp4}\")\n",
        "    os.remove(output_avi)\n",
        "else:\n",
        "    print(f\"Saved as AVI: {output_avi}\")\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n\")\n",
        "print(\"           YOLO-SwinV2 DETECTION SUMMARY (WITH TRACKING)\")\n",
        "print(f\"\\n\")\n",
        "print(f\"Total frames processed: {frame_index}\")\n",
        "print(f\"Total detections: {sum(metrics['detections_per_frame'])}\")\n",
        "print(f\"Avg detections/frame: {np.mean(metrics['detections_per_frame']):.2f}\")\n",
        "if metrics[\"confidence_scores\"]:\n",
        "    print(f\"Mean confidence: {np.mean(metrics['confidence_scores']):.4f}\")\n",
        "print(f\"Total cars: {sum(metrics['class_counts']['car'])}\")\n",
        "print(f\"Total trucks: {sum(metrics['class_counts']['truck'])}\")\n",
        "print(f\"Total buses: {sum(metrics['class_counts']['bus'])}\")\n",
        "print(f\"Unique tracks created: {next_track_id}\")\n",
        "print(f\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b14661d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the video with tracking\n",
        "print(\"YOLO-SwinV2 Vehicle Detection with Tracking:\\n\")\n",
        "\n",
        "if os.path.exists(output_mp4):\n",
        "    file_size_mb = os.path.getsize(output_mp4) / 1024 / 1024\n",
        "    print(f\"Video: {output_mp4} ({file_size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Compress for display if large\n",
        "    if file_size_mb > 20:\n",
        "        compressed = os.path.join(OUTPUT_DIR, 'tracking_compressed.mp4')\n",
        "        os.system(f'ffmpeg -y -i \"{output_mp4}\" -vcodec libx264 -crf 28 -preset fast -vf scale=640:-2 \"{compressed}\" -loglevel error')\n",
        "        if os.path.exists(compressed):\n",
        "            video_to_show = compressed\n",
        "            print(f\"Compressed for display: {os.path.getsize(compressed)/1024/1024:.1f} MB\")\n",
        "        else:\n",
        "            video_to_show = output_mp4\n",
        "    else:\n",
        "        video_to_show = output_mp4\n",
        "    \n",
        "    video_data = open(video_to_show, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "    display(HTML(f'''\n",
        "        <video width=\"800\" controls autoplay muted>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    '''))\n",
        "else:\n",
        "    print(f\"Video not found: {output_mp4}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd78ccb",
      "metadata": {},
      "source": [
        "## Metrics Visualization and Comparison with YOLO-V5m\n",
        "\n",
        "Visualize inference metrics and compare with the pre-trained YOLO-V5m baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56132b04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# METRICS VISUALIZATION AND COMPARISON\n",
        "\n",
        "viz_dir = os.path.join(OUTPUT_DIR, 'visualizations')\n",
        "os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "# Load YOLO-V5m metrics for comparison\n",
        "yolov5m_metrics_path = PROJECT_ROOT / 'outputs' / 'YOLO_V5m' / 'YOLO_V5m_metrics.json'\n",
        "yolov5m_metrics = None\n",
        "\n",
        "if os.path.exists(yolov5m_metrics_path):\n",
        "    with open(yolov5m_metrics_path) as f:\n",
        "        yolov5m_metrics = json.load(f)\n",
        "    print(f\"Loaded YOLO-V5m metrics from: {yolov5m_metrics_path}\")\n",
        "else:\n",
        "    print(f\"YOLO-V5m metrics not found at: {yolov5m_metrics_path}\")\n",
        "    print(\"   Run YOLO_V5m.ipynb first for comparison.\")\n",
        "\n",
        "# Individual Metric Plots\n",
        "\n",
        "# Detections per Frame\n",
        "fig1, ax1 = plt.subplots(figsize=(12, 5))\n",
        "ax1.plot(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], \n",
        "         color='#E63946', linewidth=1.5, label='YOLO-SwinV2')\n",
        "ax1.fill_between(metrics[\"frame_indices\"], metrics[\"detections_per_frame\"], \n",
        "                 alpha=0.3, color='#E63946')\n",
        "if yolov5m_metrics:\n",
        "    ax1.plot(yolov5m_metrics['per_frame_data']['frame_indices'], \n",
        "             yolov5m_metrics['per_frame_data']['detections_per_frame'],\n",
        "             color='#2E86AB', linewidth=1.5, alpha=0.7, label='YOLO-V5m')\n",
        "ax1.set_xlabel('Frame Index')\n",
        "ax1.set_ylabel('Number of Detections')\n",
        "ax1.set_title('Detections per Frame Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(viz_dir, 'detections_per_frame_comparison.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Confidence Score Distribution\n",
        "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "if metrics[\"confidence_scores\"]:\n",
        "    ax2.hist(metrics[\"confidence_scores\"], bins=30, color='#E63946', \n",
        "             edgecolor='white', alpha=0.7, label='YOLO-SwinV2')\n",
        "    ax2.axvline(np.mean(metrics[\"confidence_scores\"]), color='#E63946', \n",
        "                linestyle='--', linewidth=2, \n",
        "                label=f'SwinV2 Mean: {np.mean(metrics[\"confidence_scores\"]):.3f}')\n",
        "\n",
        "if yolov5m_metrics and yolov5m_metrics['summary']['mean_confidence'] > 0:\n",
        "    ax2.axvline(yolov5m_metrics['summary']['mean_confidence'], color='#2E86AB', \n",
        "                linestyle='--', linewidth=2,\n",
        "                label=f'V5m Mean: {yolov5m_metrics[\"summary\"][\"mean_confidence\"]:.3f}')\n",
        "ax2.set_xlabel('Confidence Score')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Confidence Score Distribution')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(viz_dir, 'confidence_distribution.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Class Distribution Over Time\n",
        "fig3, ax3 = plt.subplots(figsize=(12, 5))\n",
        "frames = metrics[\"frame_indices\"]\n",
        "ax3.stackplot(frames, \n",
        "              metrics[\"class_counts\"][\"car\"],\n",
        "              metrics[\"class_counts\"][\"truck\"],\n",
        "              metrics[\"class_counts\"][\"bus\"],\n",
        "              labels=['Car', 'Truck', 'Bus'],\n",
        "              colors=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)\n",
        "ax3.legend(loc='upper right')\n",
        "ax3.set_xlabel('Frame Index')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.set_title('YOLO-SwinV2: Detection Count by Class')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(viz_dir, 'class_distribution_over_time.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Model Comparison Summary\n",
        "if yolov5m_metrics:\n",
        "    fig4, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Total Detections\n",
        "    models = ['YOLO-V5m', 'YOLO-SwinV2']\n",
        "    total_dets = [yolov5m_metrics['summary']['total_detections'],\n",
        "                  sum(metrics['detections_per_frame'])]\n",
        "    colors = ['#2E86AB', '#E63946']\n",
        "    axes[0].bar(models, total_dets, color=colors)\n",
        "    axes[0].set_ylabel('Total Detections')\n",
        "    axes[0].set_title('Total Detections Comparison')\n",
        "    for i, v in enumerate(total_dets):\n",
        "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "    \n",
        "    # Mean Confidence\n",
        "    mean_confs = [yolov5m_metrics['summary']['mean_confidence'],\n",
        "                  np.mean(metrics['confidence_scores']) if metrics['confidence_scores'] else 0]\n",
        "    axes[1].bar(models, mean_confs, color=colors)\n",
        "    axes[1].set_ylabel('Mean Confidence')\n",
        "    axes[1].set_title('Mean Confidence Comparison')\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    for i, v in enumerate(mean_confs):\n",
        "        axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Class Distribution\n",
        "    x = np.arange(3)\n",
        "    width = 0.35\n",
        "    v5m_classes = [yolov5m_metrics['summary']['total_cars'],\n",
        "                   yolov5m_metrics['summary']['total_trucks'],\n",
        "                   yolov5m_metrics['summary']['total_buses']]\n",
        "    swin_classes = [sum(metrics['class_counts']['car']),\n",
        "                    sum(metrics['class_counts']['truck']),\n",
        "                    sum(metrics['class_counts']['bus'])]\n",
        "    axes[2].bar(x - width/2, v5m_classes, width, label='YOLO-V5m', color='#2E86AB')\n",
        "    axes[2].bar(x + width/2, swin_classes, width, label='YOLO-SwinV2', color='#E63946')\n",
        "    axes[2].set_xticks(x)\n",
        "    axes[2].set_xticklabels(['Car', 'Truck', 'Bus'])\n",
        "    axes[2].set_ylabel('Count')\n",
        "    axes[2].set_title('Detection Count by Class')\n",
        "    axes[2].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(viz_dir, 'model_comparison.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# Print Comparison Summary\n",
        "\n",
        "print(f\"\\n\")\n",
        "print(\"                    MODEL COMPARISON SUMMARY\")\n",
        "print(f\"\\n\")\n",
        "print(f\"{'Metric':<30} {'YOLO-V5m':>15} {'YOLO-SwinV2':>15}\")\n",
        "print(f\"\\n\")\n",
        "\n",
        "if yolov5m_metrics:\n",
        "    print(f\"{'Total Frames':<30} {yolov5m_metrics['summary']['total_frames']:>15} {len(metrics['frame_indices']):>15}\")\n",
        "    print(f\"{'Total Detections':<30} {yolov5m_metrics['summary']['total_detections']:>15} {sum(metrics['detections_per_frame']):>15}\")\n",
        "    print(f\"{'Avg Detections/Frame':<30} {yolov5m_metrics['summary']['avg_detections_per_frame']:>15.2f} {np.mean(metrics['detections_per_frame']):>15.2f}\")\n",
        "    print(f\"{'Mean Confidence':<30} {yolov5m_metrics['summary']['mean_confidence']:>15.4f} {np.mean(metrics['confidence_scores']) if metrics['confidence_scores'] else 0:>15.4f}\")\n",
        "    print(f\"{'Total Cars':<30} {yolov5m_metrics['summary']['total_cars']:>15} {sum(metrics['class_counts']['car']):>15}\")\n",
        "    print(f\"{'Total Trucks':<30} {yolov5m_metrics['summary']['total_trucks']:>15} {sum(metrics['class_counts']['truck']):>15}\")\n",
        "    print(f\"{'Total Buses':<30} {yolov5m_metrics['summary']['total_buses']:>15} {sum(metrics['class_counts']['bus']):>15}\")\n",
        "else:\n",
        "    print(f\"{'Total Frames':<30} {'N/A':>15} {len(metrics['frame_indices']):>15}\")\n",
        "    print(f\"{'Total Detections':<30} {'N/A':>15} {sum(metrics['detections_per_frame']):>15}\")\n",
        "    print(f\"{'Avg Detections/Frame':<30} {'N/A':>15} {np.mean(metrics['detections_per_frame']):>15.2f}\")\n",
        "\n",
        "print(f\"\\n\")\n",
        "print(f\"Visualizations saved to: {viz_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6819099",
      "metadata": {},
      "source": [
        "## Export YOLO-SwinV2 Metrics\n",
        "\n",
        "Save the inference metrics to JSON for future reference and comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507de7aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export YOLO-SwinV2 metrics to JSON\n",
        "metrics_export = {\n",
        "    \"model_name\": \"YOLO-SwinV2 (Trained on AAU RainSnow)\",\n",
        "    \"config\": {\n",
        "        \"conf_threshold\": CONF_THRESH,\n",
        "        \"iou_match_threshold\": IOU_MATCH_THRESH,\n",
        "        \"vehicle_classes\": list(VEHICLE_CLASSES.values()),\n",
        "        \"weights_path\": WEIGHTS_PATH,\n",
        "    },\n",
        "    \"summary\": {\n",
        "        \"total_frames\": len(metrics[\"frame_indices\"]),\n",
        "        \"total_detections\": sum(metrics[\"detections_per_frame\"]),\n",
        "        \"avg_detections_per_frame\": float(np.mean(metrics[\"detections_per_frame\"])),\n",
        "        \"mean_confidence\": float(np.mean(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"std_confidence\": float(np.std(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"min_confidence\": float(min(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"max_confidence\": float(max(metrics[\"confidence_scores\"])) if metrics[\"confidence_scores\"] else 0,\n",
        "        \"total_cars\": sum(metrics[\"class_counts\"][\"car\"]),\n",
        "        \"total_trucks\": sum(metrics[\"class_counts\"][\"truck\"]),\n",
        "        \"total_buses\": sum(metrics[\"class_counts\"][\"bus\"]),\n",
        "        \"unique_tracks\": next_track_id,\n",
        "    },\n",
        "    \"per_frame_data\": {\n",
        "        \"frame_indices\": metrics[\"frame_indices\"],\n",
        "        \"detections_per_frame\": metrics[\"detections_per_frame\"],\n",
        "        \"class_counts\": metrics[\"class_counts\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "metrics_path = os.path.join(OUTPUT_DIR, \"YOLO_SwinV2_metrics.json\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_export, f, indent=2)\n",
        "\n",
        "print(f\"Metrics exported to: {metrics_path}\")\n",
        "print(f\"\\nYou can compare these metrics with YOLO-V5m results.\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  - Video with tracking: {output_mp4}\")\n",
        "print(f\"  - Metrics JSON: {metrics_path}\")\n",
        "print(f\"  - Visualizations: {viz_dir}/\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
