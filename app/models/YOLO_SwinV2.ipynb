{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c361898",
   "metadata": {},
   "source": [
    "# Vehicle Detection with YOLO-SwinV2 Model\n",
    "\n",
    "This notebook implements a modified YOLO model with SwinV2-Tiny as the backbone for vehicle detection. The model is trained on the AAU RainSnow dataset (vehicles in rainy/snowy conditions) and evaluated on the same highway video as the pre-trained YOLO-V5m model.\n",
    "\n",
    "**Key Features:**\n",
    "- SwinV2-Tiny backbone replacing the original CSPDarknet backbone\n",
    "- Training on weather-degraded vehicle data for improved detection in adverse conditions\n",
    "- Metrics collection for comparison with the pre-trained YOLO-V5m baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e15d03",
   "metadata": {},
   "source": [
    "## How To Run\n",
    "\n",
    "It is recommended to run this notebook in Google Colab. However, it is implemented so that it can also be run in a local environment.\n",
    "\n",
    "**To run this notebook in Google Colab:**\n",
    "- Download the whole project folder (enhanced_vehicle_detection) from GitHub.\n",
    "- Place it in MyDrive in Google Drive.\n",
    "    - If the project folder is placed in a different path in Google Drive, the paths for the input video and outputs need to be edited accordingly.\n",
    "- All set! You can now run the cells.\n",
    "\n",
    "**To run this notebook in a local environment:**\n",
    "- Fork or clone the GitHub repository.\n",
    "- Run `pip install -r app/requirements.txt` to install all required libraries.\n",
    "- Since the code requires video conversion, make sure to install **ffmpeg**:\n",
    "    - macOS: `brew install ffmpeg`\n",
    "    - Ubuntu/Linux: `sudo apt install ffmpeg`\n",
    "    - Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html)\n",
    "- All set! You can now run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60300e",
   "metadata": {},
   "source": [
    "## Setup YOLO V5 \n",
    "\n",
    "The code below installs every required libraries to load and use YOLO-V5 model. This code only need to be run once while using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -q https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!pip -q install -r requirements.txt opencv-python-headless==4.10.0.84 timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b7125",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML, display\n",
    "from base64 import b64encode\n",
    "import timm\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b7ced",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up paths based on whether running in Google Colab or local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2bf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Paths for Colab\n",
    "    DATA_ROOT = '/content/drive/MyDrive/enhanced_vehicle_detection/data/training_data_vehicles_in_rain'\n",
    "    VIDEO_PATH = '/content/drive/MyDrive/enhanced_vehicle_detection/data/rainy_highway_video.mp4'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/enhanced_vehicle_detection/outputs/YOLO_SwinV2'\n",
    "    YOLOV5M_METRICS_PATH = '/content/drive/MyDrive/enhanced_vehicle_detection/outputs/YOLO_V5m/YOLO_V5m_metrics.json'\n",
    "else:\n",
    "    # Paths for local environment\n",
    "    DATA_ROOT = '../data/training_data_vehicles_in_rain'\n",
    "    VIDEO_PATH = '../data/rainy_highway_video.mp4'\n",
    "    OUTPUT_DIR = '../outputs/YOLO_SwinV2'\n",
    "    YOLOV5M_METRICS_PATH = '../outputs/YOLO_V5m/YOLO_V5m_metrics.json'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'visualizations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'checkpoints'), exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM SEED FOR REPRODUCIBILITY\n",
    "# ============================================================================\n",
    "SEED = 42  # Change this value to get different reproducible results\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    \n",
    "    # For deterministic behavior (may slow down training slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for additional reproducibility\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Random seed set to: {SEED}\")\n",
    "# ============================================================================\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb0666",
   "metadata": {},
   "source": [
    "## Training Hyperparameters\n",
    "\n",
    "**Modify these values to adjust training behavior.** For a class project with ~90 min time limit, use 10-20 epochs.\n",
    "\n",
    "| Parameter | Description | Recommended Range |\n",
    "|-----------|-------------|-------------------|\n",
    "| `num_epochs` | Number of training epochs | 10-20 for quick training, 50+ for better results |\n",
    "| `batch_size` | Samples per batch | 8-32 (lower if GPU memory limited) |\n",
    "| `learning_rate` | Initial learning rate | 1e-3 to 1e-4 |\n",
    "| `optimizer` | Optimization algorithm | 'sgd' (faster) or 'adamw' (better convergence) |\n",
    "| `img_size` | Input image size | **256** (required for SwinV2-Tiny window8_256) |\n",
    "\n",
    "> **Note:** The SwinV2-Tiny model uses `swinv2_tiny_window8_256` which requires 256x256 input images. This also speeds up training compared to 640x640."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING HYPERPARAMETERS - MODIFY THESE VALUES AS NEEDED\n",
    "# ============================================================================\n",
    "\n",
    "# Training parameters (adjust for training time vs. performance trade-off)\n",
    "NUM_EPOCHS = 15          # Number of epochs (10-20 for ~90 min training, 50+ for better results)\n",
    "BATCH_SIZE = 16          # Batch size (reduce to 8 if GPU memory is limited)\n",
    "LEARNING_RATE = 1e-3     # Initial learning rate (1e-3 for SGD, 1e-4 for AdamW)\n",
    "WEIGHT_DECAY = 0.01      # Weight decay for regularization\n",
    "\n",
    "# Optimizer selection: 'sgd' for faster training, 'adamw' for better convergence\n",
    "OPTIMIZER = 'sgd'        # Options: 'sgd', 'adamw'\n",
    "\n",
    "# SGD-specific parameters (used only if OPTIMIZER = 'sgd')\n",
    "SGD_MOMENTUM = 0.937     # Momentum for SGD optimizer\n",
    "\n",
    "# Image and model parameters\n",
    "# NOTE: SwinV2-Tiny (window8_256) requires input size of 256\n",
    "# Using 256x256 also speeds up training significantly\n",
    "IMG_SIZE = 256           # Input image size (must be 256 for swinv2_tiny_window8_256)\n",
    "\n",
    "# Detection thresholds\n",
    "# Note: Use lower confidence for models trained from scratch\n",
    "CONF_THRESH = 0.25       # Confidence threshold for detection (0.25-0.4 for trained models)\n",
    "IOU_THRESH = 0.45        # IoU threshold for NMS\n",
    "IOU_MATCH_THRESH = 0.3   # IoU threshold for tracking\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Build CONFIG dictionary from hyperparameters\n",
    "CONFIG = {\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'optimizer': OPTIMIZER,\n",
    "    'sgd_momentum': SGD_MOMENTUM,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'conf_thresh': CONF_THRESH,\n",
    "    'iou_thresh': IOU_THRESH,\n",
    "    'iou_match_thresh': IOU_MATCH_THRESH,\n",
    "    'vehicle_classes': {'car', 'truck', 'bus'},\n",
    "    'num_classes': 3,  # car, truck, bus\n",
    "    'seed': SEED,  # For reproducibility tracking\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 50)\n",
    "print(\"        TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Epochs:         {CONFIG['num_epochs']}\")\n",
    "print(f\"  Batch Size:     {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning Rate:  {CONFIG['learning_rate']}\")\n",
    "print(f\"  Optimizer:      {CONFIG['optimizer'].upper()}\")\n",
    "print(f\"  Image Size:     {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"  Conf Threshold: {CONFIG['conf_thresh']}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b8c82",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load and prepare the AAU RainSnow dataset for training. The dataset is in COCO format and contains vehicle annotations for images captured in rain and snow conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO category IDs for vehicles\n",
    "COCO_VEHICLE_CATS = {3: 'car', 6: 'bus', 8: 'truck'}\n",
    "# Map COCO IDs to our class indices\n",
    "COCO_TO_CLASS = {3: 0, 6: 1, 8: 2}  # car=0, bus=1, truck=2\n",
    "\n",
    "def load_coco_annotations(json_path):\n",
    "    \"\"\"Load COCO format annotations and filter for vehicle classes.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create image_id to filename mapping\n",
    "    images = {img['id']: img for img in coco_data['images']}\n",
    "    \n",
    "    # Group annotations by image\n",
    "    annotations_by_image = defaultdict(list)\n",
    "    for ann in coco_data['annotations']:\n",
    "        if ann['category_id'] in COCO_VEHICLE_CATS:\n",
    "            annotations_by_image[ann['image_id']].append(ann)\n",
    "    \n",
    "    return images, annotations_by_image\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    \"\"\"Dataset for vehicle detection from AAU RainSnow dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, img_size=640, split='train', train_ratio=0.8):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        \n",
    "        # Load annotations\n",
    "        json_path = self.data_root / 'aauRainSnow-rgb.json'\n",
    "        self.images, self.annotations = load_coco_annotations(json_path)\n",
    "        \n",
    "        # Get list of image IDs with vehicle annotations\n",
    "        self.image_ids = [img_id for img_id in self.annotations.keys() \n",
    "                         if len(self.annotations[img_id]) > 0]\n",
    "        \n",
    "        # Split into train/val\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(self.image_ids)\n",
    "        split_idx = int(len(self.image_ids) * train_ratio)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.image_ids = self.image_ids[:split_idx]\n",
    "        else:\n",
    "            self.image_ids = self.image_ids[split_idx:]\n",
    "        \n",
    "        print(f\"{split} set: {len(self.image_ids)} images with vehicle annotations\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # Construct image path\n",
    "        img_path = self.data_root / img_info['file_name']\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            # Return a blank image if not found\n",
    "            img = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "            return self._preprocess_image(img), torch.zeros((0, 5))\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        \n",
    "        # Resize image\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.annotations[img_id]\n",
    "        targets = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            if ann['category_id'] not in COCO_TO_CLASS:\n",
    "                continue\n",
    "            \n",
    "            class_id = COCO_TO_CLASS[ann['category_id']]\n",
    "            bbox = ann['bbox']  # [x, y, width, height]\n",
    "            \n",
    "            # Convert to normalized [x_center, y_center, width, height]\n",
    "            x_center = (bbox[0] + bbox[2] / 2) / orig_w\n",
    "            y_center = (bbox[1] + bbox[3] / 2) / orig_h\n",
    "            width = bbox[2] / orig_w\n",
    "            height = bbox[3] / orig_h\n",
    "            \n",
    "            targets.append([class_id, x_center, y_center, width, height])\n",
    "        \n",
    "        img_tensor = self._preprocess_image(img)\n",
    "        \n",
    "        if len(targets) > 0:\n",
    "            targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        else:\n",
    "            targets = torch.zeros((0, 5))\n",
    "        \n",
    "        return img_tensor, targets\n",
    "    \n",
    "    def _preprocess_image(self, img):\n",
    "        \"\"\"Normalize and convert image to tensor.\"\"\"\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)  # HWC -> CHW\n",
    "        return img\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable number of targets.\"\"\"\n",
    "    imgs, targets = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    \n",
    "    # Add batch index to targets\n",
    "    batch_targets = []\n",
    "    for i, t in enumerate(targets):\n",
    "        if len(t) > 0:\n",
    "            batch_idx = torch.full((len(t), 1), i)\n",
    "            batch_targets.append(torch.cat([batch_idx, t], dim=1))\n",
    "    \n",
    "    if len(batch_targets) > 0:\n",
    "        batch_targets = torch.cat(batch_targets, 0)\n",
    "    else:\n",
    "        batch_targets = torch.zeros((0, 6))\n",
    "    \n",
    "    return imgs, batch_targets\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "print(\"Loading dataset...\")\n",
    "train_dataset = VehicleDataset(DATA_ROOT, img_size=CONFIG['img_size'], split='train')\n",
    "val_dataset = VehicleDataset(DATA_ROOT, img_size=CONFIG['img_size'], split='val')\n",
    "\n",
    "# Create a generator with the seed for reproducible shuffling\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"Ensure each DataLoader worker has a deterministic seed.\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                          shuffle=True, collate_fn=collate_fn, num_workers=2,\n",
    "                          worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                        shuffle=False, collate_fn=collate_fn, num_workers=2,\n",
    "                        worker_init_fn=seed_worker)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afe2ef",
   "metadata": {},
   "source": [
    "## Dataset Visualization\n",
    "\n",
    "Visualize sample images with their ground truth annotations to verify the data loading pipeline is working correctly. This helps identify any issues with:\n",
    "- Image loading and preprocessing\n",
    "- Bounding box coordinate conversion\n",
    "- Class label mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc958e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with annotations to verify data loading\n",
    "print(\"Visualizing sample images with ground truth annotations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "CLASS_COLORS = {0: (255, 0, 0), 1: (0, 255, 0), 2: (0, 0, 255)}  # car=red, bus=green, truck=blue\n",
    "CLASS_NAMES_VIZ = {0: 'car', 1: 'bus', 2: 'truck'}\n",
    "\n",
    "# Get a few samples from the training dataset\n",
    "sample_indices = np.random.choice(len(train_dataset), min(6, len(train_dataset)), replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    img_tensor, targets = train_dataset[idx]\n",
    "    \n",
    "    # Convert tensor to numpy image\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()  # CHW -> HWC\n",
    "    img = (img * 255).astype(np.uint8).copy()\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    img_size = CONFIG['img_size']\n",
    "    for target in targets:\n",
    "        cls_id, x_center, y_center, width, height = target.numpy()\n",
    "        cls_id = int(cls_id)\n",
    "        \n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        x1 = int((x_center - width / 2) * img_size)\n",
    "        y1 = int((y_center - height / 2) * img_size)\n",
    "        x2 = int((x_center + width / 2) * img_size)\n",
    "        y2 = int((y_center + height / 2) * img_size)\n",
    "        \n",
    "        # Clip to image boundaries\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(img_size, x2), min(img_size, y2)\n",
    "        \n",
    "        color = CLASS_COLORS.get(cls_id, (255, 255, 255))\n",
    "        # Convert RGB to BGR for cv2, then back to RGB for display\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(img, CLASS_NAMES_VIZ.get(cls_id, 'unknown'), (x1, y1 - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Sample {idx}: {len(targets)} annotations\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Training Dataset Samples with Ground Truth Annotations', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'visualizations', 'dataset_samples.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"           DATASET STATISTICS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "\n",
    "# Count total annotations\n",
    "total_train_annotations = 0\n",
    "class_counts = {'car': 0, 'bus': 0, 'truck': 0}\n",
    "for i in range(min(len(train_dataset), 100)):  # Sample first 100\n",
    "    _, targets = train_dataset[i]\n",
    "    total_train_annotations += len(targets)\n",
    "    for t in targets:\n",
    "        cls_id = int(t[0].item())\n",
    "        cls_name = CLASS_NAMES_VIZ.get(cls_id, 'unknown')\n",
    "        if cls_name in class_counts:\n",
    "            class_counts[cls_name] += 1\n",
    "\n",
    "print(f\"\\nAnnotations in first 100 training images:\")\n",
    "print(f\"  - Cars: {class_counts['car']}\")\n",
    "print(f\"  - Buses: {class_counts['bus']}\")\n",
    "print(f\"  - Trucks: {class_counts['truck']}\")\n",
    "print(f\"  - Total: {total_train_annotations}\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fba211",
   "metadata": {},
   "source": [
    "## SwinV2 Backbone Implementation\n",
    "\n",
    "The SwinV2-Tiny (Swin Transformer V2) is used as the backbone to replace the original CSPDarknet backbone in YOLO. SwinV2 uses shifted window attention mechanism which is more effective for visual recognition tasks.\n",
    "\n",
    "**Key advantages of SwinV2:**\n",
    "- Hierarchical feature representation with multi-scale outputs\n",
    "- Efficient window-based self-attention\n",
    "- Better handling of various object scales\n",
    "- Pre-trained on ImageNet for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43656d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinV2Backbone(nn.Module):\n",
    "    \"\"\"SwinV2-Tiny backbone for feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained SwinV2-Tiny\n",
    "        self.swin = timm.create_model('swinv2_tiny_window8_256', \n",
    "                                       pretrained=pretrained,\n",
    "                                       features_only=True,\n",
    "                                       out_indices=(1, 2, 3))  # Multi-scale outputs\n",
    "        \n",
    "        # Get actual output channels from the model\n",
    "        # SwinV2-Tiny output channels at different stages\n",
    "        self.out_channels = self.swin.feature_info.channels()\n",
    "        print(f\"SwinV2 output channels: {self.out_channels}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get multi-scale features\n",
    "        features = self.swin(x)\n",
    "        \n",
    "        # SwinV2 outputs in (B, H, W, C) format, convert to (B, C, H, W) for conv layers\n",
    "        features = [f.permute(0, 3, 1, 2).contiguous() for f in features]\n",
    "        \n",
    "        return features  # List of [P3, P4, P5] features in NCHW format\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with BatchNorm and SiLU activation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    \"\"\"YOLO detection head for a single scale.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes, num_anchors=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        # Output: (num_anchors * (5 + num_classes)) per grid cell\n",
    "        # 5 = x, y, w, h, objectness\n",
    "        out_channels = num_anchors * (5 + num_classes)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, in_channels, 3, padding=1),\n",
    "            ConvBlock(in_channels, in_channels, 3, padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class YOLOSwinV2(nn.Module):\n",
    "    \"\"\"YOLO model with SwinV2-Tiny backbone.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=3, pretrained_backbone=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = 3\n",
    "        \n",
    "        # SwinV2 Backbone\n",
    "        self.backbone = SwinV2Backbone(pretrained=pretrained_backbone)\n",
    "        \n",
    "        # Get actual channel sizes from backbone\n",
    "        backbone_channels = self.backbone.out_channels\n",
    "        print(f\"Backbone output channels: {backbone_channels}\")\n",
    "        \n",
    "        # Neck: Feature Pyramid Network (FPN) style\n",
    "        # Adapt SwinV2 channels to YOLO neck channels\n",
    "        self.adapt_p3 = ConvBlock(backbone_channels[0], 256, 1)   # P3\n",
    "        self.adapt_p4 = ConvBlock(backbone_channels[1], 512, 1)   # P4\n",
    "        self.adapt_p5 = ConvBlock(backbone_channels[2], 1024, 1)  # P5\n",
    "        \n",
    "        # Upsample for FPN\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Lateral connections\n",
    "        self.lateral_p4 = ConvBlock(512 + 1024, 512, 1)\n",
    "        self.lateral_p3 = ConvBlock(256 + 512, 256, 1)\n",
    "        \n",
    "        # Detection heads for each scale\n",
    "        self.head_p3 = DetectionHead(256, num_classes, self.num_anchors)\n",
    "        self.head_p4 = DetectionHead(512, num_classes, self.num_anchors)\n",
    "        self.head_p5 = DetectionHead(1024, num_classes, self.num_anchors)\n",
    "        \n",
    "        # Anchors for each scale (scaled for 256x256 input)\n",
    "        # Original anchors were for 640x640, scaled by 256/640 = 0.4\n",
    "        self.anchors = torch.tensor([\n",
    "            [[4, 5], [6, 12], [13, 9]],      # P3 (small objects)\n",
    "            [[12, 24], [25, 18], [24, 48]],  # P4 (medium objects)\n",
    "            [[46, 36], [62, 79], [149, 130]] # P5 (large objects)\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Backbone features\n",
    "        p3, p4, p5 = self.backbone(x)\n",
    "        \n",
    "        # Adapt channels\n",
    "        p3 = self.adapt_p3(p3)\n",
    "        p4 = self.adapt_p4(p4)\n",
    "        p5 = self.adapt_p5(p5)\n",
    "        \n",
    "        # FPN top-down pathway\n",
    "        p4 = self.lateral_p4(torch.cat([p4, self.upsample(p5)], dim=1))\n",
    "        p3 = self.lateral_p3(torch.cat([p3, self.upsample(p4)], dim=1))\n",
    "        \n",
    "        # Detection outputs\n",
    "        out_p3 = self.head_p3(p3)\n",
    "        out_p4 = self.head_p4(p4)\n",
    "        out_p5 = self.head_p5(p5)\n",
    "        \n",
    "        return [out_p3, out_p4, out_p5]\n",
    "    \n",
    "    def decode_predictions(self, outputs, conf_thresh=0.5, img_size=640):\n",
    "        \"\"\"Decode raw outputs to bounding boxes.\"\"\"\n",
    "        batch_size = outputs[0].shape[0]\n",
    "        all_boxes = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            boxes = []\n",
    "            \n",
    "            for scale_idx, output in enumerate(outputs):\n",
    "                _, _, h, w = output.shape\n",
    "                stride = img_size // h\n",
    "                \n",
    "                pred = output[batch_idx].view(self.num_anchors, 5 + self.num_classes, h, w)\n",
    "                pred = pred.permute(0, 2, 3, 1).contiguous()\n",
    "                \n",
    "                # Get objectness and class scores\n",
    "                obj = torch.sigmoid(pred[..., 4])\n",
    "                cls_scores = torch.sigmoid(pred[..., 5:])\n",
    "                \n",
    "                # Find high confidence predictions\n",
    "                mask = obj > conf_thresh\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get coordinates\n",
    "                for anchor_idx in range(self.num_anchors):\n",
    "                    for yi in range(h):\n",
    "                        for xi in range(w):\n",
    "                            if mask[anchor_idx, yi, xi]:\n",
    "                                tx, ty = pred[anchor_idx, yi, xi, :2]\n",
    "                                tw, th = pred[anchor_idx, yi, xi, 2:4]\n",
    "                                \n",
    "                                # Decode bbox\n",
    "                                x = (torch.sigmoid(tx) + xi) * stride\n",
    "                                y = (torch.sigmoid(ty) + yi) * stride\n",
    "                                w_box = torch.exp(tw) * self.anchors[scale_idx, anchor_idx, 0]\n",
    "                                h_box = torch.exp(th) * self.anchors[scale_idx, anchor_idx, 1]\n",
    "                                \n",
    "                                conf = obj[anchor_idx, yi, xi]\n",
    "                                cls_score, cls_id = cls_scores[anchor_idx, yi, xi].max(0)\n",
    "                                \n",
    "                                x1 = x - w_box / 2\n",
    "                                y1 = y - h_box / 2\n",
    "                                x2 = x + w_box / 2\n",
    "                                y2 = y + h_box / 2\n",
    "                                \n",
    "                                boxes.append([x1.item(), y1.item(), x2.item(), y2.item(), \n",
    "                                            (conf * cls_score).item(), cls_id.item()])\n",
    "                \n",
    "            all_boxes.append(boxes)\n",
    "        \n",
    "        return all_boxes\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing YOLO-SwinV2 model...\")\n",
    "model = YOLOSwinV2(num_classes=CONFIG['num_classes'], pretrained_backbone=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219616c9",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the YOLO-SwinV2 model on the AAU RainSnow dataset. The training loop tracks:\n",
    "- Training loss per epoch\n",
    "- Validation loss per epoch\n",
    "- Number of detections\n",
    "- Learning rate schedule\n",
    "\n",
    "The trained model is saved to the checkpoints directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40aa503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    \"\"\"YOLO loss function with proper target assignment.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=3, num_anchors=3, img_size=256):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        # Anchors scaled for 256x256 input (same as model)\n",
    "        self.anchors = torch.tensor([\n",
    "            [[4, 5], [6, 12], [13, 9]],      # P3 (stride 8)\n",
    "            [[12, 24], [25, 18], [24, 48]],  # P4 (stride 16)\n",
    "            [[46, 36], [62, 79], [149, 130]] # P5 (stride 32)\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        self.strides = [8, 16, 32]  # Feature map strides\n",
    "        \n",
    "        # Loss weights\n",
    "        self.lambda_obj = 1.0\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_box = 5.0\n",
    "        self.lambda_cls = 1.0\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: list of tensors [P3, P4, P5]\n",
    "        targets: tensor of shape [N, 6] where each row is [batch_idx, class_id, x, y, w, h]\n",
    "        \"\"\"\n",
    "        device = predictions[0].device\n",
    "        batch_size = predictions[0].shape[0]\n",
    "        \n",
    "        total_obj_loss = torch.tensor(0.0, device=device)\n",
    "        total_box_loss = torch.tensor(0.0, device=device)\n",
    "        total_cls_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        num_pos = 0\n",
    "        \n",
    "        for scale_idx, pred in enumerate(predictions):\n",
    "            _, _, h, w = pred.shape\n",
    "            stride = self.strides[scale_idx]\n",
    "            anchors = self.anchors[scale_idx].to(device)\n",
    "            \n",
    "            # Reshape prediction\n",
    "            pred = pred.view(batch_size, self.num_anchors, 5 + self.num_classes, h, w)\n",
    "            pred = pred.permute(0, 1, 3, 4, 2).contiguous()\n",
    "            \n",
    "            # Create target tensors\n",
    "            obj_mask = torch.zeros(batch_size, self.num_anchors, h, w, device=device)\n",
    "            noobj_mask = torch.ones(batch_size, self.num_anchors, h, w, device=device)\n",
    "            tx = torch.zeros(batch_size, self.num_anchors, h, w, device=device)\n",
    "            ty = torch.zeros(batch_size, self.num_anchors, h, w, device=device)\n",
    "            tw = torch.zeros(batch_size, self.num_anchors, h, w, device=device)\n",
    "            th = torch.zeros(batch_size, self.num_anchors, h, w, device=device)\n",
    "            tcls = torch.zeros(batch_size, self.num_anchors, h, w, device=device, dtype=torch.long)\n",
    "            \n",
    "            if len(targets) > 0:\n",
    "                for target in targets:\n",
    "                    b = int(target[0].item())\n",
    "                    cls_id = int(target[1].item())\n",
    "                    gx, gy, gw, gh = target[2:6]\n",
    "                    \n",
    "                    # Convert to feature map coordinates\n",
    "                    gx_fm = gx.item() * w\n",
    "                    gy_fm = gy.item() * h\n",
    "                    gi = min(max(int(gx_fm), 0), w - 1)\n",
    "                    gj = min(max(int(gy_fm), 0), h - 1)\n",
    "                    \n",
    "                    # Convert to pixel size\n",
    "                    gw_px = gw.item() * self.img_size\n",
    "                    gh_px = gh.item() * self.img_size\n",
    "                    \n",
    "                    # Find best anchor\n",
    "                    anchor_ious = []\n",
    "                    for a_idx in range(self.num_anchors):\n",
    "                        aw, ah = anchors[a_idx]\n",
    "                        inter_w = min(gw_px, aw.item())\n",
    "                        inter_h = min(gh_px, ah.item())\n",
    "                        inter = inter_w * inter_h\n",
    "                        union = gw_px * gh_px + aw.item() * ah.item() - inter\n",
    "                        anchor_ious.append(inter / (union + 1e-6))\n",
    "                    \n",
    "                    best_anchor = int(np.argmax(anchor_ious))\n",
    "                    \n",
    "                    # Set masks and targets\n",
    "                    obj_mask[b, best_anchor, gj, gi] = 1\n",
    "                    noobj_mask[b, best_anchor, gj, gi] = 0\n",
    "                    tx[b, best_anchor, gj, gi] = gx_fm - gi\n",
    "                    ty[b, best_anchor, gj, gi] = gy_fm - gj\n",
    "                    aw, ah = anchors[best_anchor]\n",
    "                    tw[b, best_anchor, gj, gi] = torch.log(torch.tensor(gw_px / aw.item() + 1e-6))\n",
    "                    th[b, best_anchor, gj, gi] = torch.log(torch.tensor(gh_px / ah.item() + 1e-6))\n",
    "                    tcls[b, best_anchor, gj, gi] = cls_id\n",
    "                    num_pos += 1\n",
    "            \n",
    "            # Compute losses\n",
    "            pred_obj = pred[..., 4]\n",
    "            pred_xy = torch.sigmoid(pred[..., :2])\n",
    "            pred_wh = pred[..., 2:4]\n",
    "            pred_cls = pred[..., 5:]\n",
    "            \n",
    "            # Objectness loss\n",
    "            obj_loss = self.bce(pred_obj, obj_mask)\n",
    "            total_obj_loss += self.lambda_obj * (obj_loss * obj_mask).sum()\n",
    "            total_obj_loss += self.lambda_noobj * (obj_loss * noobj_mask).sum()\n",
    "            \n",
    "            # Box and class loss for positive samples\n",
    "            if obj_mask.sum() > 0:\n",
    "                box_loss_x = (self.mse(pred_xy[..., 0], tx) * obj_mask).sum()\n",
    "                box_loss_y = (self.mse(pred_xy[..., 1], ty) * obj_mask).sum()\n",
    "                box_loss_w = (self.mse(pred_wh[..., 0], tw.to(device)) * obj_mask).sum()\n",
    "                box_loss_h = (self.mse(pred_wh[..., 1], th.to(device)) * obj_mask).sum()\n",
    "                total_box_loss += self.lambda_box * (box_loss_x + box_loss_y + box_loss_w + box_loss_h)\n",
    "                \n",
    "                pred_cls_flat = pred_cls[obj_mask == 1]\n",
    "                tcls_flat = tcls[obj_mask == 1]\n",
    "                if len(pred_cls_flat) > 0:\n",
    "                    total_cls_loss += self.lambda_cls * self.ce(pred_cls_flat, tcls_flat).sum()\n",
    "        \n",
    "        num_pos = max(num_pos, 1)\n",
    "        total_loss = (total_obj_loss + total_box_loss + total_cls_loss) / num_pos\n",
    "        \n",
    "        return total_loss, {\n",
    "            'obj_loss': total_obj_loss.item() / num_pos,\n",
    "            'box_loss': total_box_loss.item() / num_pos,\n",
    "            'cls_loss': total_cls_loss.item() / num_pos\n",
    "        }\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        \n",
    "        loss, loss_items = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss, _ = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "criterion = YOLOLoss(num_classes=CONFIG['num_classes'], img_size=CONFIG['img_size'])\n",
    "\n",
    "# Select optimizer based on CONFIG\n",
    "if CONFIG['optimizer'].lower() == 'sgd':\n",
    "    # SGD with momentum - faster training, good for quick experiments\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=CONFIG['learning_rate'],\n",
    "        momentum=CONFIG['sgd_momentum'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "        nesterov=True\n",
    "    )\n",
    "    print(f\"Using SGD optimizer with momentum={CONFIG['sgd_momentum']}\")\n",
    "else:\n",
    "    # AdamW - better convergence, recommended for final training\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=CONFIG['learning_rate'], \n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    print(\"Using AdamW optimizer\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])\n",
    "\n",
    "# Training metrics storage\n",
    "training_metrics = {\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'learning_rates': [],\n",
    "    'epochs': []\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting training for {CONFIG['num_epochs']} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Store metrics\n",
    "    training_metrics['train_losses'].append(train_loss)\n",
    "    training_metrics['val_losses'].append(val_loss)\n",
    "    training_metrics['learning_rates'].append(current_lr)\n",
    "    training_metrics['epochs'].append(epoch + 1)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{CONFIG['num_epochs']}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, 'checkpoints', 'best_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  -> Saved best model (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "# Save final model\n",
    "final_checkpoint_path = os.path.join(OUTPUT_DIR, 'checkpoints', 'final_model.pth')\n",
    "torch.save({\n",
    "    'epoch': CONFIG['num_epochs'],\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'training_metrics': training_metrics,\n",
    "}, final_checkpoint_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Training complete! Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Models saved to: {os.path.join(OUTPUT_DIR, 'checkpoints')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e4082",
   "metadata": {},
   "source": [
    "## Training Metrics Visualization\n",
    "\n",
    "Visualize the training progress including:\n",
    "- Training and validation loss curves\n",
    "- Learning rate schedule\n",
    "- Comparison with YOLO-V5m baseline (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_dir = os.path.join(OUTPUT_DIR, 'visualizations')\n",
    "\n",
    "# Plot Training Metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('YOLO-SwinV2 Training Metrics', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "ax1 = axes[0]\n",
    "ax1.plot(training_metrics['epochs'], training_metrics['train_losses'], \n",
    "         'b-', linewidth=2, label='Train Loss')\n",
    "ax1.plot(training_metrics['epochs'], training_metrics['val_losses'], \n",
    "         'r-', linewidth=2, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Learning Rate Schedule\n",
    "ax2 = axes[1]\n",
    "ax2.plot(training_metrics['epochs'], training_metrics['learning_rates'], \n",
    "         'g-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss Convergence (Log Scale)\n",
    "ax3 = axes[2]\n",
    "ax3.semilogy(training_metrics['epochs'], training_metrics['train_losses'], \n",
    "             'b-', linewidth=2, label='Train Loss')\n",
    "ax3.semilogy(training_metrics['epochs'], training_metrics['val_losses'], \n",
    "             'r-', linewidth=2, label='Val Loss')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss (log scale)')\n",
    "ax3.set_title('Loss Convergence (Log Scale)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_dir, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print Training Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"         YOLO-SwinV2 Training Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTotal Epochs: {len(training_metrics['epochs'])}\")\n",
    "print(f\"Final Train Loss: {training_metrics['train_losses'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {training_metrics['val_losses'][-1]:.4f}\")\n",
    "print(f\"Best Val Loss: {min(training_metrics['val_losses']):.4f}\")\n",
    "print(f\"Best Epoch: {training_metrics['val_losses'].index(min(training_metrics['val_losses'])) + 1}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save training metrics to JSON\n",
    "training_export = {\n",
    "    'model_name': 'YOLO-SwinV2',\n",
    "    'config': {k: str(v) if isinstance(v, set) else v for k, v in CONFIG.items()},\n",
    "    'training_metrics': training_metrics,\n",
    "    'best_val_loss': min(training_metrics['val_losses']),\n",
    "    'final_train_loss': training_metrics['train_losses'][-1],\n",
    "    'final_val_loss': training_metrics['val_losses'][-1],\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_training_metrics.json'), 'w') as f:\n",
    "    json.dump(training_export, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining metrics saved to: {os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_training_metrics.json')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba561e50",
   "metadata": {},
   "source": [
    "## Video Detection and Comparison\n",
    "\n",
    "Apply the trained YOLO-SwinV2 model to the test video and compare detection metrics with the pre-trained YOLO-V5m baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "best_model_path = os.path.join(OUTPUT_DIR, 'checkpoints', 'best_model.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Detection metrics storage\n",
    "inference_metrics = {\n",
    "    'frame_indices': [],\n",
    "    'detections_per_frame': [],\n",
    "    'confidence_scores': [],\n",
    "    'class_counts': {'car': [], 'truck': [], 'bus': []},\n",
    "}\n",
    "\n",
    "CLASS_NAMES = {0: 'car', 1: 'bus', 2: 'truck'}\n",
    "\n",
    "# Process video\n",
    "print(f\"\\nProcessing video: {VIDEO_PATH}\")\n",
    "\n",
    "if os.path.exists(VIDEO_PATH):\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        raise RuntimeError(\"Couldn't read the video\")\n",
    "    \n",
    "    h, w = frame.shape[:2]\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 15\n",
    "    \n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(OUTPUT_DIR, \"YOLO_SwinV2_highway_with_detection.avi\"),\n",
    "        cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "        fps,\n",
    "        (w, h)\n",
    "    )\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    frame_index = 0\n",
    "    \n",
    "    pbar = tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), desc='Processing video')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_index += 1\n",
    "            \n",
    "            # Preprocess frame\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, (CONFIG['img_size'], CONFIG['img_size']))\n",
    "            img_tensor = torch.from_numpy(img_resized).float().permute(2, 0, 1) / 255.0\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(img_tensor)\n",
    "            boxes = model.decode_predictions(outputs, conf_thresh=CONFIG['conf_thresh'], \n",
    "                                             img_size=CONFIG['img_size'])[0]\n",
    "            \n",
    "            # Collect metrics\n",
    "            inference_metrics['frame_indices'].append(frame_index)\n",
    "            inference_metrics['detections_per_frame'].append(len(boxes))\n",
    "            \n",
    "            class_count = {'car': 0, 'truck': 0, 'bus': 0}\n",
    "            \n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls_id = box\n",
    "                \n",
    "                # Scale to original frame size\n",
    "                scale_x = w / CONFIG['img_size']\n",
    "                scale_y = h / CONFIG['img_size']\n",
    "                x1, x2 = int(x1 * scale_x), int(x2 * scale_x)\n",
    "                y1, y2 = int(y1 * scale_y), int(y2 * scale_y)\n",
    "                \n",
    "                # Clip to frame boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(w, x2), min(h, y2)\n",
    "                \n",
    "                cls_name = CLASS_NAMES.get(int(cls_id), 'car')\n",
    "                class_count[cls_name] += 1\n",
    "                inference_metrics['confidence_scores'].append(conf)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{cls_name} ({conf:.2f})\", (x1, y1 - 7),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            \n",
    "            for cls in class_count:\n",
    "                inference_metrics['class_counts'][cls].append(class_count[cls])\n",
    "            \n",
    "            out.write(frame)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Convert to MP4\n",
    "    output_video_path = os.path.join(OUTPUT_DIR, 'highway_with_detection.mp4')\n",
    "    avi_path = os.path.join(OUTPUT_DIR, \"YOLO_SwinV2_highway_with_detection.avi\")\n",
    "    \n",
    "    os.system(f'ffmpeg -y -i \"{avi_path}\" -vcodec libx264 -crf 23 -pix_fmt yuv420p \"{output_video_path}\" >/dev/null 2>&1')\n",
    "    \n",
    "    print(f\"\\nVideo saved to: {output_video_path}\")\n",
    "    print(f\"Total frames processed: {frame_index}\")\n",
    "    print(f\"Total detections: {sum(inference_metrics['detections_per_frame'])}\")\n",
    "    \n",
    "    # Display the video in the notebook\n",
    "    print(\"\\nVehicle Detection Video for YOLO-SwinV2:\")\n",
    "    video_mp4 = open(output_video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_mp4).decode()\n",
    "    display(HTML(f'<video width=640 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>'))\n",
    "else:\n",
    "    print(f\"Video not found at: {VIDEO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3641a",
   "metadata": {},
   "source": [
    "## Model Comparison: YOLO-SwinV2 vs YOLO-V5m\n",
    "\n",
    "Compare the inference metrics of the trained YOLO-SwinV2 model with the pre-trained YOLO-V5m baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO-V5m metrics for comparison\n",
    "yolov5m_metrics = None\n",
    "if os.path.exists(YOLOV5M_METRICS_PATH):\n",
    "    with open(YOLOV5M_METRICS_PATH, 'r') as f:\n",
    "        yolov5m_metrics = json.load(f)\n",
    "    print(\"Loaded YOLO-V5m metrics for comparison\")\n",
    "else:\n",
    "    print(f\"YOLO-V5m metrics not found at: {YOLOV5M_METRICS_PATH}\")\n",
    "    print(\"Run YOLO_V5m.ipynb first to generate baseline metrics\")\n",
    "\n",
    "# Calculate YOLO-SwinV2 summary metrics\n",
    "swinv2_summary = {\n",
    "    'total_frames': len(inference_metrics['frame_indices']),\n",
    "    'total_detections': sum(inference_metrics['detections_per_frame']),\n",
    "    'avg_detections_per_frame': np.mean(inference_metrics['detections_per_frame']) if inference_metrics['detections_per_frame'] else 0,\n",
    "    'mean_confidence': np.mean(inference_metrics['confidence_scores']) if inference_metrics['confidence_scores'] else 0,\n",
    "    'std_confidence': np.std(inference_metrics['confidence_scores']) if inference_metrics['confidence_scores'] else 0,\n",
    "    'total_cars': sum(inference_metrics['class_counts']['car']),\n",
    "    'total_trucks': sum(inference_metrics['class_counts']['truck']),\n",
    "    'total_buses': sum(inference_metrics['class_counts']['bus']),\n",
    "}\n",
    "\n",
    "# Create comparison visualization\n",
    "if yolov5m_metrics:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Model Comparison: YOLO-SwinV2 vs YOLO-V5m', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Total Detections Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    models = ['YOLO-V5m', 'YOLO-SwinV2']\n",
    "    detections = [yolov5m_metrics['summary']['total_detections'], swinv2_summary['total_detections']]\n",
    "    bars = ax1.bar(models, detections, color=['#2E86AB', '#A23B72'])\n",
    "    ax1.set_ylabel('Total Detections')\n",
    "    ax1.set_title('Total Detections Comparison')\n",
    "    for bar, val in zip(bars, detections):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                f'{val}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Confidence Score Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    v5m_conf = [yolov5m_metrics['summary']['mean_confidence'], \n",
    "                yolov5m_metrics['summary']['std_confidence']]\n",
    "    swinv2_conf = [swinv2_summary['mean_confidence'], swinv2_summary['std_confidence']]\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    ax2.bar(x - width/2, v5m_conf, width, label='YOLO-V5m', color='#2E86AB')\n",
    "    ax2.bar(x + width/2, swinv2_conf, width, label='YOLO-SwinV2', color='#A23B72')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(['Mean Conf', 'Std Conf'])\n",
    "    ax2.set_ylabel('Confidence Score')\n",
    "    ax2.set_title('Confidence Score Comparison')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Class Distribution Comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    x = np.arange(3)\n",
    "    v5m_classes = [yolov5m_metrics['summary']['total_cars'],\n",
    "                   yolov5m_metrics['summary']['total_trucks'],\n",
    "                   yolov5m_metrics['summary']['total_buses']]\n",
    "    swinv2_classes = [swinv2_summary['total_cars'],\n",
    "                      swinv2_summary['total_trucks'],\n",
    "                      swinv2_summary['total_buses']]\n",
    "    ax3.bar(x - width/2, v5m_classes, width, label='YOLO-V5m', color='#2E86AB')\n",
    "    ax3.bar(x + width/2, swinv2_classes, width, label='YOLO-SwinV2', color='#A23B72')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(['Cars', 'Trucks', 'Buses'])\n",
    "    ax3.set_ylabel('Total Count')\n",
    "    ax3.set_title('Detection by Class')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Detections per Frame Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    if len(inference_metrics['frame_indices']) > 0:\n",
    "        # Use min length for comparison\n",
    "        min_len = min(len(yolov5m_metrics['per_frame_data']['frame_indices']), \n",
    "                      len(inference_metrics['frame_indices']))\n",
    "        ax4.plot(yolov5m_metrics['per_frame_data']['frame_indices'][:min_len],\n",
    "                yolov5m_metrics['per_frame_data']['detections_per_frame'][:min_len],\n",
    "                'b-', alpha=0.7, label='YOLO-V5m')\n",
    "        ax4.plot(inference_metrics['frame_indices'][:min_len],\n",
    "                inference_metrics['detections_per_frame'][:min_len],\n",
    "                'r-', alpha=0.7, label='YOLO-SwinV2')\n",
    "    ax4.set_xlabel('Frame')\n",
    "    ax4.set_ylabel('Detections')\n",
    "    ax4.set_title('Detections per Frame')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'model_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print Comparison Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"                    MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Metric':<30} {'YOLO-V5m':<20} {'YOLO-SwinV2':<20}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Total Frames':<30} {yolov5m_metrics['summary']['total_frames']:<20} {swinv2_summary['total_frames']:<20}\")\n",
    "    print(f\"{'Total Detections':<30} {yolov5m_metrics['summary']['total_detections']:<20} {swinv2_summary['total_detections']:<20}\")\n",
    "    print(f\"{'Avg Detections/Frame':<30} {yolov5m_metrics['summary']['avg_detections_per_frame']:<20.2f} {swinv2_summary['avg_detections_per_frame']:<20.2f}\")\n",
    "    print(f\"{'Mean Confidence':<30} {yolov5m_metrics['summary']['mean_confidence']:<20.4f} {swinv2_summary['mean_confidence']:<20.4f}\")\n",
    "    print(f\"{'Total Cars':<30} {yolov5m_metrics['summary']['total_cars']:<20} {swinv2_summary['total_cars']:<20}\")\n",
    "    print(f\"{'Total Trucks':<30} {yolov5m_metrics['summary']['total_trucks']:<20} {swinv2_summary['total_trucks']:<20}\")\n",
    "    print(f\"{'Total Buses':<30} {yolov5m_metrics['summary']['total_buses']:<20} {swinv2_summary['total_buses']:<20}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Export YOLO-SwinV2 inference metrics\n",
    "swinv2_export = {\n",
    "    'model_name': 'YOLO-SwinV2 (Trained)',\n",
    "    'config': {k: str(v) if isinstance(v, set) else v for k, v in CONFIG.items()},\n",
    "    'summary': swinv2_summary,\n",
    "    'per_frame_data': {\n",
    "        'frame_indices': inference_metrics['frame_indices'],\n",
    "        'detections_per_frame': inference_metrics['detections_per_frame'],\n",
    "        'class_counts': inference_metrics['class_counts']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_metrics.json'), 'w') as f:\n",
    "    json.dump(swinv2_export, f, indent=2)\n",
    "\n",
    "print(f\"\\nYOLO-SwinV2 metrics exported to: {os.path.join(OUTPUT_DIR, 'YOLO_SwinV2_metrics.json')}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
